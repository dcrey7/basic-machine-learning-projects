{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":976879,"sourceType":"datasetVersion","datasetId":533871}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EXAM\n\n**Objective:** classify a job posting as Frauduent or Legit.\nIn this dataset, a job posting is a combination of all the textual columns. Thus, you need to regroup these information together. \n\n1. Split the train set into Training,  Validation, & Test sets (0.75, 0.15, 0.1).\n2. Exploratory Data Analysis (EDA): look closely at your data, the textual columns, the target, any null values, any imbalance, non-numerical data to be converted (in data and target)?, etc.\n3. Clean Textual Data\n4. Vectorize Texts (one hot encoder, tfidf, embeddings, etc.)\n5. Choose and justify the choice of the evaluation metric\n6. ML Classification model(s) or DL model or an ensemble of several ML/DL or both ML and DL models\n7. Hyperparameter tuning (Cross validation - hyperopt - Gridsearch), if you use crossvalidation, then no need to split the train set into train and validation. If you use hyperopt, you will need both sets. NB: Once you have identified the optimal hyperparameters, you must merge the train adn validation sets together and retrain the model on both datasets before predicting on the test set.\n8. Evaluate on Test set (i.e., apply the cleaning, the vectorization, and the model)\n9. Use Lime to explain one text classification\n10. Use Shapley to explain globally the text classification\n\n**Comment all your steps: justify your choices, analyze the results (whether positive or negative), and finally conclude by suggesting improvements and making recommendations.**\n\nYou can use one notebook for all your work or separate the EDA from the training and finally the testing and interpretability. <br>\nYou can also use .py scripts (classes/functions) and initialize/call them from the notebook. <br>\n\nyou will be evaluated on the problem solving skills, the methods used, and **the comments and analysis** you will provide. <br>\nOrganize the notebook and follow the steps as listed above, if you would like to change the order of the sections, you will need to justify the reason. <br>\n\n**NB:**\n-Imbalance can be fixed by Over/Under sampling and Data Augmentation.<br> Using Textual Data Augmentation techniques would be a bonus if used.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Introduction**\n\nIn this notebook, I explore the challenging problem of detecting fraudulent job postings using machine learning and natural language processing techniques. With the proliferation of online job platforms, identifying deceptive listings has become increasingly important to protect job seekers from scams that could lead to identity theft, financial loss, or wasted time pursuing non-existent opportunities.\n\n**Table of Contents**\n\n1. **Train, Validation, & Test split**\n\n2. **Exploratory Data Analysis**\n   - Words in fraud & non fraud posting\n   - Distributions of fraudulent for each binary feature\n   - Distribution of employment type, required experience\n   - Fraudulent vs Non fraudulent in each country\n\n3. **Pre-processing**\n   - Make preprocessing pipeline for validation and test\n\n4. **Sampling and data augmentation methods**\n\n5. **Modelling (Ensemble and traditional)**\n\n6. **Hyperparameter tuning**\n\n7. **Interpreting model**\n\n9. **Deep learning modelling (BERT)**\n\n10. **BERT Interpretation**\n\n11. **Conclusion**\n\nAs shown in the table of contents, this project follows a comprehensive approach to fraud detection, beginning with data splitting and exploratory analysis, moving through preprocessing and sampling techniques, and culminating in both traditional modeling and advanced deep learning with BERT. Each section builds upon previous findings to create a robust classification system capable of distinguishing legitimate job postings from fraudulent ones.\n\nThroughout the notebook, I've structured my work with detailed commentary following a consistent format:\n- **What the code does so far?** - Technical explanation of implementation details\n- **Why do we need to do this?** - Rationale behind each approach and methodology\n- **Analysis and insights** - Key findings and patterns discovered in the data\n- **Things to keep in mind** - Important considerations, limitations, and potential improvements\n\nFor this project, I used the \"Real or Fake Job Posting Prediction\" dataset from Kaggle (https://www.kaggle.com/datasets/shivamb/real-or-fake-fake-jobposting-prediction), which contains approximately 18,000 job postings with around 800 fraudulent examples. Due to the computational demands of transformer-based models like BERT, I executed this notebook in Kaggle's environment to leverage their GPU capabilities.\n\nI would like to acknowledge that I utilized ChatGPT for assistance with certain code syntax optimizations and to create cleaner, more interpretable visualizations. This collaborative approach allowed me to focus on the analytical aspects while maintaining high-quality code and presentation standards.\n\nBy the conclusion of this notebook, I aim to not only build effective classification models but also provide actionable insights into the linguistic and structural patterns that differentiate legitimate job opportunities from fraudulent schemes.","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:28:48.198463Z","iopub.status.busy":"2025-05-06T12:28:48.197760Z","iopub.status.idle":"2025-05-06T12:28:59.028622Z","shell.execute_reply":"2025-05-06T12:28:59.027705Z","shell.execute_reply.started":"2025-05-06T12:28:48.198442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Import necessary libraries\n\n# import pandas as pd\n# import numpy as np\n# import warnings\n# import torch\n# import random\n\n# import shap\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n# from sklearn.linear_model import LogisticRegression\n# import xgboost as xgb\n# import lightgbm as lgb\n\n# import torch\n# import numpy as np\n# import pandas as pd\n# import time, random\n# import matplotlib.pyplot as plt\n# from datetime import datetime\n\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier\n# import xgboost as xgb\n# import lightgbm as lgb\n# from catboost import CatBoostClassifier\n# from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n# import pandas as pd\n# import numpy as np\n\n# # from sklearn.model_selection import train_test_split\n# from lime.lime_text import LimeTextExplainer\n# import torch\n# # Ignore warnings\n# warnings.filterwarnings('ignore')\n\n# # Set display options for pandas\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n# pd.set_option('display.width', 1000)\n\n# # Set all random seeds for reproducibility\n# random.seed(42)\n# np.random.seed(42)\n# torch.manual_seed(42)\n# torch.cuda.manual_seed_all(42)\n\n# # Set device (GPU if available, otherwise CPU)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import FunctionTransformer\n\n\n# import shap\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n# from sklearn.linear_model import LogisticRegression\n# import xgboost as xgb\n# import lightgbm as lgb\n\n# # Essential data science modules\n# import matplotlib.pyplot as plt\n# import seaborn as sb\n# sb.set()\n\n# # Natural Language Toolkit (NTLK) modules\n# from lime.lime_text import LimeTextExplainer\n# !pip install imblearn\n# !pip install -U scikit-learn imbalanced-learn\n\n# !pip install nltk stop-words\n# import nltk\n# nltk.download('stopwords')\n# from nltk import pos_tag\n# from nltk.stem import WordNetLemmatizer\n# from nltk.corpus import stopwords, wordnet\n# nltk.download(['stopwords', 'averaged_perceptron_tagger', 'wordnet', 'punkt'])\n\n# # scikit-learn modules\n# from sklearn.model_selection import train_test_split, GridSearchCV\n# from sklearn.dummy import DummyClassifier\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef, confusion_matrix\n# from sklearn.metrics import classification_report, f1_score, fbeta_score\n# # from sklearn.metrics import plot_roc_curve\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n# from sklearn.svm import SVC\n# from sklearn.ensemble import RandomForestClassifier\n# import pandas as pd\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from imblearn.over_sampling import RandomOverSampler, SMOTE\n# from imblearn.under_sampling import RandomUnderSampler\n# import nltk\n# from nltk.corpus import wordnet\n# import random\n\n# # Other modules\n# # !pip install stop_words eli5 re\n# # from stop_words import get_stop_words\n# # import eli5\n# # import re\n# # from scipy import stats\n# # import matplotlib.gridspec as gridspec\n# # from wordcloud import WordCloud\n# # # from imblearn.under_sampling import RandomUnderSampler\n# # # from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN\n# # from imblearn.pipeline import Pipeline\n# # import plotly.graph_objects as go\n# # from plotly.subplots import make_subplots\n# # import spacy\n# # from spacy.lang.en import English\n# # import gc\n# # import tensorflow as tf\n# # from tensorflow import keras\n# # from tensorflow.keras.preprocessing.sequence import pad_sequences\n# # from tensorflow.keras.preprocessing.text import Tokenizer","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:28:59.030245Z","iopub.status.busy":"2025-05-06T12:28:59.029959Z","iopub.status.idle":"2025-05-06T12:29:12.105692Z","shell.execute_reply":"2025-05-06T12:29:12.105044Z","shell.execute_reply.started":"2025-05-06T12:28:59.030195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set all random seeds for reproducibility\nimport random\nrandom.seed(42)\n\n# Basic data libraries\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\nfrom datetime import datetime\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nsb.set()\n\n# PyTorch setup\nimport torch\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Scikit-learn libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, FunctionTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import (\n    confusion_matrix, precision_score, recall_score, f1_score, fbeta_score,\n    roc_auc_score, accuracy_score, matthews_corrcoef, classification_report\n)\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import SVC\n\n# Gradient boosting libraries\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# Explainability libraries\nimport shap\nfrom lime.lime_text import LimeTextExplainer\n\n# Imbalanced learning (requires separate pip install)\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# NLTK setup (requires separate pip install)\nimport nltk\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')\ndf.info()\n\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:12.107311Z","iopub.status.busy":"2025-05-06T12:29:12.106603Z","iopub.status.idle":"2025-05-06T12:29:13.644763Z","shell.execute_reply":"2025-05-06T12:29:13.644128Z","shell.execute_reply.started":"2025-05-06T12:29:12.107281Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clean_cols=[i for i in df.columns if 'Unnamed' not in i]\n# df=df[clean_cols]\n\nprint(df.shape)\ndf.info()\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.645576Z","iopub.status.busy":"2025-05-06T12:29:13.645385Z","iopub.status.idle":"2025-05-06T12:29:13.665510Z","shell.execute_reply":"2025-05-06T12:29:13.664854Z","shell.execute_reply.started":"2025-05-06T12:29:13.645559Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()/len(df)*100","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.666561Z","iopub.status.busy":"2025-05-06T12:29:13.666293Z","iopub.status.idle":"2025-05-06T12:29:13.694873Z","shell.execute_reply":"2025-05-06T12:29:13.694256Z","shell.execute_reply.started":"2025-05-06T12:29:13.666536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.697897Z","iopub.status.busy":"2025-05-06T12:29:13.697455Z","iopub.status.idle":"2025-05-06T12:29:13.717637Z","shell.execute_reply":"2025-05-06T12:29:13.716928Z","shell.execute_reply.started":"2025-05-06T12:29:13.697879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.fraudulent.value_counts(normalize=True)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.718534Z","iopub.status.busy":"2025-05-06T12:29:13.718305Z","iopub.status.idle":"2025-05-06T12:29:13.737065Z","shell.execute_reply":"2025-05-06T12:29:13.736539Z","shell.execute_reply.started":"2025-05-06T12:29:13.718516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Train, Validation, & Test split","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\ndf_train, df_validation = train_test_split(df_train, test_size=0.16, random_state=42)\n\n\nprint(f\"train shape is: {df_train.shape}\")\nprint(f\"train target distribution: {df_train.fraudulent.value_counts(normalize=True)}\")\ndisplay(df_train.head())\n\nprint(f\"validation shape is: {df_validation.shape}\")\nprint(f\"validation target distribution: {df_validation.fraudulent.value_counts(normalize=True)}\")\ndisplay(df_validation.head())\n\nprint(f\"test shape is: {df_test.shape}\")\nprint(f\"test target distribution: {df_test.fraudulent.value_counts(normalize=True)}\")\ndisplay(df_test.head())","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.738163Z","iopub.status.busy":"2025-05-06T12:29:13.737923Z","iopub.status.idle":"2025-05-06T12:29:13.795679Z","shell.execute_reply":"2025-05-06T12:29:13.794985Z","shell.execute_reply.started":"2025-05-06T12:29:13.738132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"train shape is: {df_train.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.796711Z","iopub.status.busy":"2025-05-06T12:29:13.796459Z","iopub.status.idle":"2025-05-06T12:29:13.801046Z","shell.execute_reply":"2025-05-06T12:29:13.800263Z","shell.execute_reply.started":"2025-05-06T12:29:13.796687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code does so far?**\n\nThink of us as setting the stage for a grand detective play! I've gathered a big collection of 17,880 job postings – our \"case files.\" Each file (job posting) has 18 different clues (pieces of information).\n\nTo train our \"detective\" (the model), I've smartly divided these files:\n\n- The Classroom (75% for Training): This is the largest pile where our detective learns the ropes, studying loads of examples to understand what makes a job posting real or fake.\n- The Practice Arena (15% for Validation): Here, our detective gets to fine-tune its skills on job postings it hasn't seen in class, helping us adjust its methods.\n- The Final Exam (10% for Testing): This is the ultimate test! The detective faces completely new job postings to see how Ill it performs in a real-world-like scenario.\nCrucially, in each of these groups, the proportion of real to fake jobs is the same – about 95% are genuine, and a sneaky 5% are fraudulent. This ensures our detective is trained and tested on a realistic mix.\n\n**2. Why are I doing this?**\n\nOur main goal is to build a super-smart system that acts like an online job posting \"fraud detector.\" Imagine you're Browse for jobs online; I want to create a tool that can flag those suspicious listings that might be scams, helping you (and everyone!) steer clear and focus on genuine opportunities. It’s all about making the job hunt safer and more reliable.\n\n**3. Analysis and insights**\n\n- The Mystery of the Missing Details: When I sift through our data, it's clear that not every job posting is a complete picture. For instance, a huge 84% of listings don't mention a salary range! But hold on, this isn't necessarily a red flag. It's pretty common in the job world for companies to keep salary details hush-hush initially to attract a wider range of candidates. The most reliable clues I usually have are tucked away in the job titles, detailed descriptions, and (if provided) the company profiles. So, a blank salary field doesn't automatically shout \"scam!\"\n- The \"5% Challenge\" - Finding the Needle in the Haystack: Here’s where it gets tricky: only about 5 out of every 100 job postings in our dataset are actually fraudulent. That means for every fake job, there are roughly 20 legitimate ones. This is what I call a \"class imbalance,\" and it’s like trying to find a few specific bad apples in a massive orchard of good ones. If our detective isn't trained carefully, it might just learn to say \"legitimate\" all the time and still be right 95% of the time – which isn't helpful for catching those crafty fakes!\n- Hallmarks of a Real Job: Looking at the genuine job postings, they often share some common traits: clear and specific job titles (e.g., \"Senior Marketing Manager\" rather than \"AMAZING HOME OPPORTUNITY!\"), concrete locations, professional language in the descriptions, and specific requirements like \"5+ years experience in Java.\" Sometimes, they even list actual salary bands or benefits. These are the kinds of positive patterns our detective will learn to recognize.\n\n**4. Things to keep in mind**\n\n- Battling the Imbalance: That tiny 5% of fraudulent jobs is a big hurdle. I’ll need to use some clever tricks to make sure our model doesn’t just ignore them. This could mean digitally creating more examples of fake jobs for it to study (oversampling), or perhaps showing it fewer examples of legitimate jobs (undersampling), or even using special algorithms that are designed to give more weight to the rare, fraudulent cases.\n- Words Matter Most: Since job titles and descriptions are the most consistently filled-out parts of the postings, our fraud detection system will need to become a real expert at understanding the nuances of language used in these texts.\n- Building on Patterns: The typical features of legitimate jobs give us a solid starting point. As I develop our model, I'll be teaching it to tell the difference between these genuine characteristics and the deceptive tactics often employed in fake job ads.","metadata":{}},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\n\nSome pre-processing is done in this portion namely:\n\nFeature extraction on the presence of the text fields company_profile, description, requirements and benefits\nSplitting of location to 3 columns - country, state and city","metadata":{}},{"cell_type":"code","source":"def add_text_specification_features(df):\n    # Create a copy to avoid modifying the original dataframe\n    result_df = df.copy()\n    \n    # Define feature categories\n    bin_features = ['telecommuting', 'has_company_logo', 'has_questions']\n    cat_features = ['department', 'employment_type', 'required_experience', \n                    'required_education', 'industry', 'function']\n    text_features = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n    complex_features = ['location', 'salary_range']\n    \n    # Create binary features for whether text fields are specified\n    for feature_name in text_features[1:]:  # Skip 'title'\n        unspec_feature_name = f'{feature_name}_specified'\n        result_df[unspec_feature_name] = (~result_df[feature_name].isna()).astype('int')\n        bin_features.append(unspec_feature_name)\n    \n    return result_df\n\ntext_features = ['title', 'company_profile', 'description', 'requirements', 'benefits']\nbin_features = ['telecommuting', 'has_company_logo', 'has_questions','company_profile_specified',\t'description_specified',\t'requirements_specified',\t'benefits_specified']\ndf_train=add_text_specification_features(df_train)\ndf_train.head()[text_features + bin_features[-4:]]\nprint(f\"train shape is: {df_train.shape}\")\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.802140Z","iopub.status.busy":"2025-05-06T12:29:13.801890Z","iopub.status.idle":"2025-05-06T12:29:13.828185Z","shell.execute_reply":"2025-05-06T12:29:13.827662Z","shell.execute_reply.started":"2025-05-06T12:29:13.802113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_location_features(df):\n    \"\"\"\n    Split location column into country, state, and city columns without changing the \n    dataframe shape\n    \n    Args:\n        df: DataFrame with a 'location' column\n        \n    Returns:\n        DataFrame with added country, state, and city columns\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    \n    # Create a copy to avoid modifying the original dataframe\n    df_result = df.copy()\n    \n    # Initialize country, state, and city columns with NaN\n    df_result['country'] = np.nan\n    df_result['state'] = np.nan\n    df_result['city'] = np.nan\n    \n    # Process only non-null location values\n    mask = df_result['location'].notna()\n    \n    # For rows with location data, split and assign\n    for idx in df_result[mask].index:\n        loc = df_result.loc[idx, 'location']\n        \n        # Split location by comma\n        loc_parts = loc.split(', ')\n        \n        # Assign parts to appropriate columns\n        if len(loc_parts) >= 1:\n            df_result.loc[idx, 'country'] = loc_parts[0]\n        if len(loc_parts) >= 2:\n            df_result.loc[idx, 'state'] = loc_parts[1]\n        if len(loc_parts) >= 3:\n            # Join any remaining parts for city\n            df_result.loc[idx, 'city'] = ', '.join(loc_parts[2:])\n    \n    # Fill NaN values with 'Unspecified'\n    df_result['country'] = df_result['country'].fillna('Unspecified')\n    df_result['state'] = df_result['state'].fillna('Unspecified')\n    df_result['city'] = df_result['city'].fillna('Unspecified')\n    \n    return df_result\n\n# Example usage:\ndf_train = split_location_features(df_train)\nprint(f\"train shape is: {df_train.shape}\")\n# print(f\"Shape after splitting location: {df_train_with_location.shape}\")\ndf_train[['location', 'country', 'state', 'city']].head()","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:13.829088Z","iopub.status.busy":"2025-05-06T12:29:13.828928Z","iopub.status.idle":"2025-05-06T12:29:18.422607Z","shell.execute_reply":"2025-05-06T12:29:18.421977Z","shell.execute_reply.started":"2025-05-06T12:29:13.829075Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:18.423809Z","iopub.status.busy":"2025-05-06T12:29:18.423334Z","iopub.status.idle":"2025-05-06T12:29:18.444149Z","shell.execute_reply":"2025-05-06T12:29:18.443434Z","shell.execute_reply.started":"2025-05-06T12:29:18.423784Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This description can be ignored as the data are all categorical data\ndf_train.describe()","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:18.445034Z","iopub.status.busy":"2025-05-06T12:29:18.444828Z","iopub.status.idle":"2025-05-06T12:29:18.486208Z","shell.execute_reply":"2025-05-06T12:29:18.485685Z","shell.execute_reply.started":"2025-05-06T12:29:18.445019Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code is doing?**\n\nI made functions for created new features for our dataset based on the intial analyis which we think would help us further to classify fake job posting. I created some smart new features to help catch fraud:\n\n1. **Specified Features**: I made new columns called \"company_profile_specified,\" \"description_specified,\" \"requirements_specified,\" and \"benefits_specified.\" These aren't about what's written but whether something was written at all. It's like checking if someone filled out all parts of an application form. Legitimate jobs almost always have descriptions (99.9%) but fraudulent jobs are more likely to skip company profile information.\n2. **Location Breakdown**: I splited the location into country, state, and city. This is like peeling an onion to look at each layer separately. Sometimes scammers make up location details, and this helps us spot when something doesn't make sense. For example, if someone claims a job is in \"New York, CA\" (California should have a different state code), our system can catch this error.\n\n\n\n**2. Why are we doing this?**\n\nI can analayze different trends and from different countries and cities which effect the fake and real job postings. Creating features are important steps to make sure that we get good results and understand the data thoroughly\n\n**3. Analysis and Insights**\n\nI can see that there are only 20% of them have missing values, I will see further if it any city or state majorly contribute to the fake job postings\n\n**4. Things to keep in mind**\n\nI have created functions for so that we can apply the same preprocessing steps to the validation and test sets later as well. we have remember that this will be done towards the end while modelling. We show examples of the feature engineering and preprocessing on the train and directly create a pipeline so that we can apply it on the validation and test set","metadata":{}},{"cell_type":"markdown","source":"## Words in fraud & non fraud posting\nDistribution of fraudulent and non-fraudulent job postings\nI also take a look at the distribution of text count in company_profile,description,requirements,benefits and found that all are about the same except for company_profile.","metadata":{}},{"cell_type":"code","source":"def visualize_word_counts_consistent(df, text_columns=None):\n    \"\"\"\n    Visualize word counts with consistent scaling for comparison betIen fraudulent vs real posts.\n    \"\"\"\n    # Create a copy to avoid modifying the original dataframe\n    proc_df = df.copy()\n    \n    # Define text columns if not provided\n    if text_columns is None:\n        text_columns = ['company_profile', 'description', 'requirements', 'benefits']\n    \n    # Drop rows where all text columns are NaN\n    proc_df = proc_df.dropna(subset=text_columns, how='all')\n    \n    # Fill NaN values with empty space\n    for col in text_columns:\n        proc_df[col] = proc_df[col].fillna(' ')\n    \n    # Create visualizations for each text column\n    for col in text_columns:\n        # Create figure with shared x-axis for each row\n        fig, ax = plt.subplots(2, 2, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex='row')\n        \n        # Fraudulent posts\n        num_fraud = proc_df[proc_df[\"fraudulent\"] == 1][col].str.split().map(lambda x: len(x))\n        ax[0][0].hist(num_fraud, bins=20, color='orangered')\n        ax[0][0].set_title('Fraudulent Posts')\n        ax[0][0].set_ylabel('Frequency')\n        ax[0][0].set_xticklabels('')\n        \n        # Real posts \n        num_real = proc_df[proc_df[\"fraudulent\"] == 0][col].str.split().map(lambda x: len(x))\n        ax[0][1].hist(num_real, bins=20, color='blue')\n        ax[0][1].set_title('Real Posts')\n        ax[0][1].set_ylabel('Frequency')\n        ax[0][1].set_xticklabels('')\n        \n        # Boxplots with shared x-axis\n        import seaborn as sb\n        sb.boxplot(x=num_fraud, orient='h', ax=ax[1][0])\n        ax[1][0].set_xlabel('Number of Words')\n        ax[1][0].set_title('Fraudulent - Box Plot')\n        \n        sb.boxplot(x=num_real, orient='h', ax=ax[1][1])\n        ax[1][1].set_xlabel('Number of Words') \n        ax[1][1].set_title('Real - Box Plot')\n        \n        # Set overall title\n        fig.suptitle(f'Word Count Distribution in {col.replace(\"_\", \" \").title()}', fontsize=16)\n        plt.tight_layout()\n        plt.subplots_adjust(hspace=0.1, top=0.9)\n        plt.show()\n\n# Apply the corrected function\nvisualize_word_counts_consistent(df_train)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:18.512766Z","iopub.status.busy":"2025-05-06T12:29:18.512596Z","iopub.status.idle":"2025-05-06T12:29:22.261304Z","shell.execute_reply":"2025-05-06T12:29:22.260540Z","shell.execute_reply.started":"2025-05-06T12:29:18.512752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.What the code does so far?**\n\nI've created a visualization function that analyzes word counts across different sections of job postings. The function takes our job posting dataset and generates comparative histograms and box plots for four key text fields: company profile, job description, requirements, and benefits. For each field, I display side-by-side visualizations showing the distribution of word counts in fraudulent posts (in orange-red) versus legitimate posts (in blue). I've ensured consistent scaling between the visualizations to make direct comparisons easier and added proper titles, labels, and formatting to make the outputs readable and informative.\n\n**2. Why are we doing this?**\n\nI'm analyzing word count distributions to identify potential patterns that distinguish real job postings from fraudulent ones. Text length is a surprisingly effective indicator of authenticity - legitimate companies typically invest more time crafting detailed descriptions compared to scammers who may prioritize quantity over quality. By visualizing these differences, I can determine if word count features might be valuable predictors for our classification model. This analysis helps me understand if scammers follow consistent patterns in how much they write in different sections of job postings.\n\n**3. Analysis and insights**\n\nMy visualizations reveal telling differences between real and fake job postings. In company profiles, Fake postings usually have very less word counts compared to real posts. for example in company profile, for fake posting the median is around 0 while it is around 100 words for real posts. Description, benefits, requirements all look similar in median but real jobs have higher words counts as outliers. We can say that fake job posting usually have lesser words as they would like to put in low effort and use simpler catchy words to attract desperate people. \n\n**4. Things to keep in mind**\n\nWhile word count differences are informative, I need to be careful not to oversimplify. Some legitimate startups might have brief company descriptions simply because they're new, not because they're fraudulent. Conversely, sophisticated scammers might intentionally write verbose descriptions to appear legitimate. I should consider combining these quantitative features with qualitative content analysis - examining what words are used, not just how many. Additionally, I should look for outliers that might skew my distributions and consider normalizing word counts by industry, as some sectors naturally use more technical language than others. I also want to explore if certain text sections (like benefits) might be stronger fraud indicators than others to prioritize the most discriminative features in my final model.","metadata":{}},{"cell_type":"markdown","source":"## Distributions of fraudulent for each binary feature","metadata":{}},{"cell_type":"code","source":"def visualize_binary_features_fraud_distribution(df, bin_features=None):\n    \"\"\"\n    Create pie charts showing the distribution of fraudulent posts for each binary feature.\n    Includes chi-square tests for significance with improved layout to prevent text overlap.\n    \n    Parameters:\n    - df: DataFrame containing the job posting data\n    - bin_features: List of binary features to visualize\n    \n    Returns:\n    - The original DataFrame\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import chi2_contingency\n    \n    # Use default binary features if none are provided\n    if bin_features is None:\n        bin_features = ['telecommuting', 'has_company_logo', 'has_questions',\n                       'company_profile_specified', 'description_specified',\n                       'requirements_specified', 'benefits_specified']\n    \n    # Calculate total required height based on number of features\n    # Increase height per feature for better spacing\n    height_per_feature = 5  # Increased from previous value\n    total_height = height_per_feature * len(bin_features)\n    \n    # Create the figure and grid with better spacing\n    fig = plt.figure(figsize=(20, total_height))\n    outer = gridspec.GridSpec(len(bin_features), 1, wspace=0.2, hspace=0.8)  # Increased hspace\n    \n    # For each binary feature\n    for feature_ind, feature_name in enumerate(bin_features):\n        inner = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=outer[feature_ind], \n                                                wspace=0.5, hspace=0.4)\n        \n        # Create a contingency table for chi-square test\n        contingency_table = pd.crosstab(df[feature_name], df['fraudulent'])\n        \n        # Perform chi-square test\n        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n        \n        # Determine significance\n        if p_value < 0.05:\n            significance_text = f\"Significant (p={p_value:.4f} < 0.05)\"\n            significance_color = 'green'\n        else:\n            significance_text = f\"Not Significant (p={p_value:.4f} > 0.05)\"\n            significance_color = 'red'\n        \n        # Create a title for this feature with significance test result\n        ax = plt.Subplot(fig, outer[feature_ind])\n        ax.set_title(f'Distribution of fraudulent for {feature_name}\\n{significance_text}', \n                    fontsize=14, color=significance_color)\n        ax.axis('off')\n        fig.add_subplot(ax)\n        \n        # Add chi-square test information in the title area\n        plt.figtext(0.5, fig.subplotpars.top - 0.02 - (0.96 / len(bin_features)) * feature_ind, \n                   f\"Chi-square: {chi2:.2f}, degrees of freedom: {dof}\", \n                   ha='center', fontsize=10)\n        \n        # Create pie charts for each value of the binary feature (0 and 1)\n        for feature_class_idx, feature_class in enumerate([0, 1]):\n            ax = plt.Subplot(fig, inner[feature_class_idx])\n            \n            # Get value counts of fraudulent for this feature class\n            feature_cl_vc = df[df[feature_name] == feature_class].fraudulent.value_counts().sort_index()\n            \n            # Set appropriate labels\n            if len(feature_cl_vc) == 2:\n                feature_cl_vc.index = ['non-fraudulent', 'fraudulent']\n            else:\n                if feature_cl_vc.index[0] == 0:\n                    feature_cl_vc.index = ['non-fraudulent']\n                else:\n                    feature_cl_vc.index = ['fraudulent']\n            \n            # Create pie chart with better text placement\n            wedges, texts, autotexts = ax.pie(\n                feature_cl_vc.values, \n                labels=None,  # Remove labels from pie itself\n                autopct='%1.1f%%',\n                colors=['#4CAF50', '#F44336'] if len(feature_cl_vc) == 2 else \n                       (['#4CAF50'] if feature_cl_vc.index[0] == 'non-fraudulent' else ['#F44336']),\n                textprops={'fontsize': 10}\n            )\n            \n            # Adjust text properties to prevent overlap\n            for autotext in autotexts:\n                autotext.set_fontsize(9)\n                autotext.set_weight('bold')\n            \n            # Add a legend instead of labels directly on pie\n            ax.legend(\n                feature_cl_vc.index,\n                loc='lower center',\n                bbox_to_anchor=(0.5, -0.15),\n                fontsize=9\n            )\n            \n            ax.set_title(f'{feature_name} = {feature_class}', fontsize=12)\n            fig.add_subplot(ax)\n    \n    # Set overall title\n    fig.suptitle('Distributions of fraudulent posts for binary features', fontsize=18, y=0.98)\n    \n    # Adjust subplot params for better spacing\n    plt.subplots_adjust(top=0.96, bottom=0.05, hspace=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # return df\n\n# Define binary features\nbin_features = ['telecommuting', 'has_company_logo', 'has_questions',\n               'company_profile_specified', 'description_specified',\n               'requirements_specified', 'benefits_specified']\n\n# Apply the function to df_train\nvisualize_binary_features_fraud_distribution(df_train, bin_features)\nprint(f\"train shape is: {df_train.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:22.262310Z","iopub.status.busy":"2025-05-06T12:29:22.262072Z","iopub.status.idle":"2025-05-06T12:29:24.547557Z","shell.execute_reply":"2025-05-06T12:29:24.546890Z","shell.execute_reply.started":"2025-05-06T12:29:22.262294Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.What the code does so far?**\n\nI've created a visualization function that generates pie charts to analyze binary (yes/no) features in our job posting dataset. For each feature like \"has_company_logo\" or \"telecommuting,\" I display separate pie charts showing the breakdown of fraudulent versus legitimate postings when that feature is present (value=1) or absent (value=0). I've enhanced the visualization with chi-square statistical tests to determine if the observed differences are statistically significant. The function automatically formats the charts with clear titles, appropriate colors (green for legitimate, red for fraudulent), and displays p-values to indicate statistical significance. I've also carefully optimized the layout to prevent text overlap and ensure readability even when visualizing multiple features simultaneously.\n\n**2.Why are we doing this?**\n\nI'm examining these binary features because they represent simple but powerful indicators that might help distinguish legitimate job postings from scams. Unlike text content which requires complex analysis, these binary signals are straightforward \"digital fingerprints\" that fraudsters might overlook or find difficult to fake consistently. By visualizing how these features distribute across real and fake postings and testing their statistical significance, I can identify which binary indicators are most reliable for my classification model. This approach is computationally efficient and provides clear, interpretable signals that complement my more complex text analysis.\n\n**3.Analysis and insights**\n\nMy visualizations reveal several compelling patterns.15% Fake jobs posting dont have logos where as only 2.1% have the logo, similarly fake job posting dont have have screensing questions , descriptions which tells us that scammers put less effort into creating a complete brand about the company. The chi-square tests confirm these aren't random patterns – the p-values below 0.05 for features like company logos, company profiles, and screening questions confirm these are statistically significant indicators of posting legitimacy. Most interestingly, almost all legitimate postings include specified requirements and descriptions, while fraudulent ones more frequently leave these sections blank.\n\n**4.Things to keep in mind**\n\nWhile these binary features provide valuable signals, I should be cautious about over-relying on any single indicator. Sophisticated scammers may eventually learn to include company logos and other elements that mimic legitimate postings. I also need to consider how these features might vary across different industries or job types – a small local business might legitimately skip having a logo while still offering real employment. Some features might be more discriminative than others, so I'll need to prioritize them based on both statistical significance and practical reliability. Moving forward, I'll combine these binary features with my text analysis for a more robust model. I should also periodically reassess these features over time, as fraudsters' tactics may evolve in response to improved detection methods","metadata":{}},{"cell_type":"markdown","source":"## Distribution of employment type, required experience, required education","metadata":{"execution":{"iopub.execute_input":"2025-04-26T00:12:36.081947Z","iopub.status.busy":"2025-04-26T00:12:36.081656Z","iopub.status.idle":"2025-04-26T00:12:36.085565Z","shell.execute_reply":"2025-04-26T00:12:36.084778Z","shell.execute_reply.started":"2025-04-26T00:12:36.081927Z"}}},{"cell_type":"code","source":"def plot_cat_feature_distribution(df, feature_name):\n    '''\n    Makes a clean matplotlib chart with categorical feature's distribution comparing \n    fraudulent vs non-fraudulent posts, with improved legend placement and text handling.\n    \n    Parameters:\n    - df: DataFrame containing the job posting data\n    - feature_name: Name of the categorical feature to visualize\n    \n    Returns:\n    - The original DataFrame\n    '''\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import chi2_contingency\n    \n    # Get value counts for the feature by fraudulent status\n    feature_0f = df[df.fraudulent == 0][feature_name].fillna('Unspecified').value_counts()\n    feature_1f = df[df.fraudulent == 1][feature_name].fillna('Unspecified').value_counts()\n    \n    # Determine if we need a large figure for many categories\n    num_categories = max(len(feature_0f), len(feature_1f))\n    has_many_categories = num_categories > 5\n    \n    # Adjust figure size based on number of categories\n    if has_many_categories:\n        fig_width = 18\n        fig_height = 10\n    else:\n        fig_width = 14\n        fig_height = 7\n    \n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(fig_width, fig_height))\n    \n    # Function to create pie chart with better legend handling\n    def create_clean_pie(ax, data, title):\n        # Fix: pie() with autopct returns 3 values (wedges, texts, autotexts)\n        wedges, _, _ = ax.pie(\n            data.values,\n            labels=None,  # Remove direct labels\n            autopct='%1.1f%%',\n            textprops={'fontsize': 9, 'weight': 'bold'},\n            colors=plt.cm.tab20.colors[:len(data)]\n        )\n        ax.set_title(title, fontsize=14)\n        return wedges\n    \n    # Create pie charts\n    wedges0 = create_clean_pie(axes[0], feature_0f, 'Non-fraudulent Posts')\n    wedges1 = create_clean_pie(axes[1], feature_1f, 'Fraudulent Posts')\n    \n    # Position legends appropriately based on number of categories\n    if has_many_categories:\n        # For many categories, place legend below the pie charts\n        axes[0].legend(\n            wedges0,\n            feature_0f.index,\n            loc='upper center', \n            bbox_to_anchor=(0.5, -0.1),\n            ncol=3,\n            fontsize=8\n        )\n        \n        axes[1].legend(\n            wedges1,\n            feature_1f.index,\n            loc='upper center', \n            bbox_to_anchor=(0.5, -0.1),\n            ncol=3,\n            fontsize=8\n        )\n    else:\n        # For fewer categories, place legend to the side\n        axes[0].legend(\n            wedges0,\n            feature_0f.index,\n            loc='center left',\n            bbox_to_anchor=(-0.3, 0.5),\n            fontsize=10\n        )\n        \n        axes[1].legend(\n            wedges1,\n            feature_1f.index,\n            loc='center right',\n            bbox_to_anchor=(1.3, 0.5),\n            fontsize=10\n        )\n    \n    # Create a contingency table for chi-square test\n    all_categories = list(set(feature_0f.index) | set(feature_1f.index))\n    contingency = np.zeros((len(all_categories), 2))\n    for i, category in enumerate(all_categories):\n        contingency[i, 0] = feature_0f.get(category, 0)\n        contingency[i, 1] = feature_1f.get(category, 0)\n    \n    # Perform chi-square test\n    chi2, p_value, dof, expected = chi2_contingency(contingency)\n    \n    # Set overall title with significance test result\n    if p_value < 0.05:\n        significance_text = f\"Significant (p={p_value:.4f} < 0.05)\"\n        significance_color = 'green'\n    else:\n        significance_text = f\"Not Significant (p={p_value:.4f} > 0.05)\"\n        significance_color = 'red'\n    \n    fig.suptitle(f'Distribution of {feature_name}\\n{significance_text}', \n                fontsize=16, color=significance_color)\n    \n    # Add chi-square test information with better placement\n    if has_many_categories:\n        y_position = 0.01\n    else:\n        y_position = 0.05\n        \n    plt.figtext(0.5, y_position, f\"Chi-square: {chi2:.2f}, degrees of freedom: {dof}\", \n                ha='center', fontsize=11)\n    \n    # Adjust spacing for the figure\n    if has_many_categories:\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.85, bottom=0.25, wspace=0.3)\n    else:\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.85, bottom=0.1, wspace=0.1)\n    \n    plt.show()\n    \n    # return df","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:24.548658Z","iopub.status.busy":"2025-05-06T12:29:24.548433Z","iopub.status.idle":"2025-05-06T12:29:24.561338Z","shell.execute_reply":"2025-05-06T12:29:24.560587Z","shell.execute_reply.started":"2025-05-06T12:29:24.548642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'employment_type' is a categorical feature in df_train\nplot_cat_feature_distribution(df_train, 'employment_type')\n\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:24.562375Z","iopub.status.busy":"2025-05-06T12:29:24.562125Z","iopub.status.idle":"2025-05-06T12:29:25.045988Z","shell.execute_reply":"2025-05-06T12:29:25.045255Z","shell.execute_reply.started":"2025-05-06T12:29:24.562349Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_cat_feature_distribution('required_experience')\nplot_cat_feature_distribution(df_train, 'required_experience')\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:25.047234Z","iopub.status.busy":"2025-05-06T12:29:25.046927Z","iopub.status.idle":"2025-05-06T12:29:25.465769Z","shell.execute_reply":"2025-05-06T12:29:25.465143Z","shell.execute_reply.started":"2025-05-06T12:29:25.047185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_cat_feature_distribution('required_education')\nplot_cat_feature_distribution(df_train, 'required_education')\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:25.470274Z","iopub.status.busy":"2025-05-06T12:29:25.470025Z","iopub.status.idle":"2025-05-06T12:29:25.988697Z","shell.execute_reply":"2025-05-06T12:29:25.988004Z","shell.execute_reply.started":"2025-05-06T12:29:25.470253Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.What the code does so far?**\n\nI've created a visualization function that generates comparative pie charts for categorical features in our job posting dataset. For each feature like \"employment_type\" or \"required_education,\" my function displays two pie charts side-by-side – one showing the distribution within legitimate posts and the other for fraudulent posts. I've built in adaptability so the visualization automatically adjusts for features with many categories by expanding the figure size and repositioning legends to maintain readability. Each visualization includes chi-square statistical testing to determine if the observed differences between legitimate and fraudulent distributions are statistically significant. The function highlights significance with color-coding (green for significant, red for non-significant) and displays the p-value prominently in the chart title.\n\n**2.Why are we doing this?**\n\nI'm analyzing these categorical features to uncover distinctive patterns that might reveal fraudulent job postings. While binary features give us yes/no signals, categorical features like employment type, required experience, and education requirements provide more nuanced insights. These categorizations represent standard fields that legitimate employers typically complete thoughtfully based on actual job requirements. By comparing how these fields are distributed between real and fake postings, I can identify subtle differences that might escape casual observation. These visualizations translate complex statistical relationships into intuitive visual patterns that help me understand how scammers approach different aspects of job posting creation compared to legitimate employers.\n\n**3.Analysis and insights**\n\nMy visualizations reveal telling patterns across categorical features. For employment types, I found that while both legitimate and fraudulent postings predominantly list \"Full-time\" positions (around 60%), fraudulent postings have a notably higher percentage of unspecified employment types (20.1% vs 16.3%). This reinforces my earlier finding that scammers often leave optional fields blank. When examining required experience, legitimate postings show a more balanced distribution across different experience levels, suggesting real employers have diverse, specific needs. Fraudulent postings, however, tend to cluster around certain experience categories, possibly indicating copy-paste behavior by scammers. The education requirements analysis shows similar patterns – both categories frequently leave education unspecified (about 55%), but when requirements are listed, there are subtle distribution differences. The statistical tests confirm these differences aren't random chance but meaningful patterns I can use for classification.\n\n**4.Things to keep in mind**\n\nWhile these categorical patterns provide valuable insights, I need to consider several important factors. First, industry differences might influence these distributions – tech jobs naturally have different education requirements than healthcare positions, so I should account for industry when interpreting these patterns. Second, I should watch for evolving fraud tactics – as scammers become more sophisticated, they might start mimicking legitimate distribution patterns more closely. Third, some legitimate small businesses or startups might have incomplete information not because they're fraudulent but because they have flexible requirements. I need to balance the signal from categorical features with other indicators to avoid false positives. Finally, I should consider combining this analysis with natural language processing of the actual text content, as the specific wording within these categories might reveal additional fraud signals that distribution analysis alone won't capture.","metadata":{}},{"cell_type":"markdown","source":"## Fraudulent vs Non fradulent in each country","metadata":{}},{"cell_type":"code","source":"def analyze_text_and_country_distribution(df):\n    \"\"\"\n    Analyzes text content and country distribution in job postings.\n    Creates visualizations comparing fraudulent vs non-fraudulent postings.\n    \n    Parameters:\n    - df: DataFrame containing job posting data\n    \n    Returns:\n    - DataFrame with added text column and country fraud statistics\n    \"\"\"\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from collections import Counter\n    import re\n    import numpy as np\n    from spacy.lang.en.stop_words import STOP_WORDS\n    \n    # Create a copy to avoid modifying the original\n    df_copy = df.copy()\n    \n    # 1. Create the combined text column\n    df_copy['text'] = df_copy['title'].fillna('') + ' ' + df_copy['location'].fillna('') + ' ' + df_copy['company_profile'].fillna('')\n    df_copy['text'] = df_copy['text'] + ' ' + df_copy['description'].fillna('') + ' ' + df_copy['requirements'].fillna('') + ' ' + df_copy['benefits'].fillna('')\n    df_copy['text'] = df_copy['text'].str.replace('\\xa0', '').str.lower()\n    \n    # 2. Extract text from fraudulent and non-fraudulent postings\n    fraud_text = ' '.join(df_copy[df_copy.fraudulent==1].text)\n    non_fraud_text = ' '.join(df_copy[df_copy.fraudulent==0].text)\n    \n    # 3. Function to get word frequencies excluding stopwords\n    def get_top_words(text, n=5):\n        # Simple word tokenization (splitting on non-alphanumeric chars)\n        words = re.findall(r'\\b[a-z]{3,}\\b', text.lower())\n        # Remove stopwords\n        words = [word for word in words if word not in STOP_WORDS]\n        # Get word counts\n        word_counts = Counter(words)\n        # Return top n words\n        return word_counts.most_common(n)\n    \n    # 4. Get top words for each category\n    fraud_top_words = get_top_words(fraud_text)\n    non_fraud_top_words = get_top_words(non_fraud_text)\n    \n    # 5. Create visualization for top words\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Fraudulent job postings chart\n    words, counts = zip(*fraud_top_words)\n    ax1.bar(words, counts, color='red')\n    ax1.set_title('Top 5 Words in Fraudulent Job Postings')\n    ax1.set_ylabel('Frequency')\n    for i, count in enumerate(counts):\n        ax1.text(i, count + 5, str(count), ha='center')\n    \n    # Non-fraudulent job postings chart\n    words, counts = zip(*non_fraud_top_words)\n    ax2.bar(words, counts, color='green')\n    ax2.set_title('Top 5 Words in Non-Fraudulent Job Postings')\n    ax2.set_ylabel('Frequency')\n    for i, count in enumerate(counts):\n        ax2.text(i, count + 5, str(count), ha='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 6. Create country visualization (top 10 countries)\n    if 'country' in df_copy.columns:\n        country = dict(df_copy.country.value_counts()[:10])\n        plt.figure(figsize=(8,6))\n        plt.title('No. of job postings from top 10 countries', size=20)\n        plt.bar(country.keys(), country.values())\n        plt.ylabel('No. of jobs', size=10)\n        plt.xlabel('Countries', size=10)\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n        \n        # 7. Create country fraud proportion visualization\n        country_meanfr_pt = pd.pivot_table(df_copy, index='country', values='fraudulent', \n                                         aggfunc=np.mean).sort_values(by='fraudulent', ascending=False)\n        country_meanfr_pt.columns = ['Proportion of fraudulent posts']\n        print('Top-10 countries with the highest proportions of fraudulent job postings:')\n        display_df = country_meanfr_pt.head(10)\n        print(display_df)\n        \n        # Create a bar chart for countries with highest fraud rates\n        plt.figure(figsize=(10, 6))\n        top_fraud_countries = country_meanfr_pt.head(10)\n        plt.bar(top_fraud_countries.index, top_fraud_countries['Proportion of fraudulent posts'], color='orangered')\n        plt.title('Top 10 Countries with Highest Proportion of Fraudulent Posts', size=16)\n        plt.xlabel('Country', size=12)\n        plt.ylabel('Proportion of Fraudulent Posts', size=12)\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n    \n    # return df_copy\n\n# Apply the function to df_train\nanalyze_text_and_country_distribution(df_train)\n\n# Display the first few rows of the resulting dataframe\n# df_train_analyzed[['text', 'fraudulent', 'country']].head()","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:26.029011Z","iopub.status.busy":"2025-05-06T12:29:26.028452Z","iopub.status.idle":"2025-05-06T12:29:32.878965Z","shell.execute_reply":"2025-05-06T12:29:32.878189Z","shell.execute_reply.started":"2025-05-06T12:29:26.028989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.What the code does so far?**\n\nI've developed a function that performs two-part analysis on our job posting dataset. First, it consolidates all textual content from various fields (title, location, company profile, description, requirements, and benefits) into a single comprehensive text column. After cleaning and normalizing this text, I extract the most frequent words from both fraudulent and legitimate postings, filtering out common stopwords to focus on meaningful terms. Second, I analyze geographic patterns by visualizing the distribution of job postings across countries and calculating fraud rates for each location. The function generates three visualizations: comparative bar charts showing top words in fraudulent versus legitimate postings, a chart displaying the volume of postings from the top 10 countries, and a focused view of the countries with the highest proportions of fraudulent listings.\n\n**2.Why are we doing this?**\n\nI'm conducting this text and geographic analysis to uncover deeper patterns that might not be apparent from structured fields alone. Word usage can reveal subtle differences in how scammers craft their postings compared to legitimate employers – certain terms or writing patterns might serve as linguistic fingerprints of fraud. Meanwhile, the geographic analysis helps identify fraud hotspots and regional variations in scammer activity. By combining textual and geographic insights, I can build more sophisticated features for my classification model that go beyond simple binary or categorical indicators. This multi-dimensional approach helps capture both the content and context of job postings, making it harder for scammers to evade detection by simply mimicking surface-level characteristics of legitimate listings.\n\n**3.Analysis and insights**\n\nMy text analysis reveals striking differences in word usage patterns. Fraudulent postings use the word \"work\" most frequently (1,307 times), followed by \"experience\" (1,173 times) and terms like \"skills,\" \"amp,\" and \"time.\" In stark contrast, legitimate postings show dramatically higher word frequencies overall, with \"experience\" appearing 28,205 times and \"work\" 26,943 times. Words like \"team,\" \"business,\" and \"company\" also appear frequently in legitimate postings. This massive disparity in word frequencies confirms my earlier finding that fraudulent postings typically contain far less textual content – legitimate employers invest in detailed descriptions while scammers often provide minimal information.\n\nThe geographic analysis reveals that while the United States dominates in posting volume (nearly 8,000 listings), followed by Great Britain and Canada, fraud rates vary dramatically by country. Bahrain shows the highest fraud proportion at 57%, followed by Malaysia at 56% and Taiwan at 50%. The United States maintains a relatively low fraud rate of 7% despite its high volume. Interestingly, some countries with fewer postings like Pakistan and Brazil show even lower fraud rates at 5% and 4% respectively. This suggests that fraud isn't simply correlated with posting volume but may reflect targeted scam operations in specific regions.\n\n**4.Things to keep in mind**\n\nWhile analyzing text frequencies provides valuable insights, I need to consider that word usage alone doesn't capture contextual nuances or phraseology that might also signal fraud. In future iterations, I should explore more sophisticated natural language processing techniques like sentiment analysis or topic modeling to better understand the semantic differences between legitimate and fraudulent content. For the geographic analysis, I need to be cautious about drawing conclusions from countries with very small sample sizes, as these might not be statistically reliable. Some fraud patterns might also reflect regional employment practices rather than scammer activity – what looks suspicious in one country might be standard practice in another.\n\nI should also consider how scammers might adapt to detection methods over time. If they become aware that detailed descriptions and specific locations reduce suspicion, they might begin crafting more elaborate texts and targeting countries with lower fraud rates. This suggests I need to periodically reassess these patterns and evolve my model accordingly. Finally, I should explore potential interactions between text content and geography – perhaps fraudulent postings from certain regions share distinctive linguistic patterns that could be leveraged for more precise detection.","metadata":{}},{"cell_type":"code","source":"print(f\"train shape is: {df_train.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:32.880070Z","iopub.status.busy":"2025-05-06T12:29:32.879636Z","iopub.status.idle":"2025-05-06T12:29:32.884152Z","shell.execute_reply":"2025-05-06T12:29:32.883398Z","shell.execute_reply.started":"2025-05-06T12:29:32.880051Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Pre-processing","metadata":{}},{"cell_type":"code","source":"def show_samples(df_original, df_cleaned=None, mode='Before'):\n    \"\"\"Function to display sample text from dataframes.\"\"\"\n    df_used = df_original\n    if mode == 'After': df_used = df_cleaned\n    print(f'--- {mode} Pre-processing (Some Samples) ---', end='\\n\\n')\n    print(df_used['description'].iloc[0], end='\\n\\n')\n    print(df_used['description'].iloc[520], end='\\n\\n')\n    print(df_used['company_profile'].iloc[381], end='\\n\\n')\n    print(df_used['benefits'].iloc[6057], end='\\n\\n')\n\ndef create_cleaned_df(df):\n    \"\"\"Function to create cleaned dataframe with transformed features.\"\"\"\n    df_cleaned = pd.DataFrame()\n    \n    # Binary and categorical features\n    df_cleaned['telecommuting'] = df['telecommuting'].replace({1: 'telecommuting_yes', 0: 'telecommuting_no'})\n    df_cleaned['country'] = ('country_' + df['country'].fillna('none')).str.lower()\n    df_cleaned['state'] = ('state_' + df['state'].fillna('none')).str.lower()\n    df_cleaned['city'] = ('city_' + df['city'].fillna('none')).str.lower()\n    df_cleaned['company_logo'] = df['has_company_logo'].replace({1: 'company_logo_yes', 0: 'company_logo_no'})\n    df_cleaned['company_profile_specified'] = df['company_profile_specified'].replace({1: 'company_profile_yes', 0: 'company_profile_no'})\n    df_cleaned['questions'] = df['has_questions'].replace({1: 'has_questions_yes', 0: 'has_questions_no'})\n    df_cleaned['employment_type'] = ('employment_type_' + df['employment_type'].str.replace(r' ', '').str.lower()).fillna('')\n    df_cleaned['required_experience'] = ('required_experience_' + df['required_experience'].str.replace(r' ', '_').str.lower()).fillna('')\n    df_cleaned['required_education'] = ('required_education_' + df['required_education'].str.replace(r' ', '_').str.lower()).fillna('')\n    df_cleaned['industry'] = ('industry_' + df['industry'].str.replace(r' ', '_').str.lower()).fillna('')\n    df_cleaned['function'] = ('function_' + df['function'].str.replace(r' ', '_').str.lower()).fillna('')\n    df_cleaned['title'] = df['title'].str.replace(r'-', '').str.lower().fillna('')\n    df_cleaned['department'] = ('department_' + df['department'].str.replace(r' ', '_').str.lower()).fillna('')\n    \n    # Text fields processing\n    text_fields = ['company_profile', 'description', 'requirements', 'benefits']\n    for item in text_fields:\n        df_cleaned[item] = (df[item].str.replace(r'\\\\n', ' ').str.replace(r'&amp', ' ').str.replace(r'\\xa0', ' ')\n                                    .str.replace(r'\\$str=',' ').str.replace(r'AppKey=*[0-9a-z]+', ' ')\n                                    .str.replace(r'#EMAIL_[\\w]+#', ' ').str.replace(r'#URL_[\\w]+#', ' ').str.replace(r'#PHONE_[\\w]+#', ' ')\n                                    .str.replace(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n                                    .str.replace(r'(?<=[a-z])(?=[A-Z])', ' ').str.replace(r'(?<=[.,?!:])(?=[A-Z])', ' ')\n                                    .str.replace(r'[\\+\\*=@#<>\\(\\)\\[\\]\\^_/\"-]+', '  ')\n                                    .str.replace(r'[^\\w\\s]*', '').str.replace(r'[0-9]+', ' ')\n                                    .str.replace(r'[\\s]+', ' ').str.lower())\n    \n    df_cleaned[text_fields] = df_cleaned[text_fields].fillna('')\n    df_cleaned['fraud'] = df['fraudulent']\n    \n    return df_cleaned\n\ndef remove_stopwords_from_df(df):\n    \"\"\"Function to remove stopwords from text fields.\"\"\"\n    from nltk.corpus import stopwords\n    from stop_words import get_stop_words\n    \n    # Create comprehensive stopword list\n    stop_words = list(get_stop_words('en'))         # About 900 stopwords\n    nltk_words = list(stopwords.words('english'))   # About 150 stopwords\n    stop_words.extend(nltk_words)\n    stop = set(stop_words)\n    \n    def remove_stopwords(sentence):\n        tokens = sentence.split()\n        tokens = [token for token in tokens if token not in stop]\n        return \" \".join(tokens)\n    \n    # Apply to text fields\n    text_fields = ['company_profile', 'description', 'requirements', 'benefits']\n    df_processed = df.copy()\n    \n    for col in text_fields:\n        df_processed[col] = df_processed[col].map(remove_stopwords)\n    \n    return df_processed\n\ndef create_final_features(df_cleaned):\n    \"\"\"Function to combine all features into a single text field.\"\"\"\n    df_end = pd.DataFrame()\n    \n    # Combine all columns except the last one (fraud)\n    df_end['text'] = df_cleaned.iloc[:, 0]\n    for col in df_cleaned.columns[1:-1]:\n        df_end['text'] = df_end['text'] + ' ' + df_cleaned[col]\n    \n    # Clean up extra spaces\n    df_end['text'] = df_end['text'].str.replace(r'[\\s]+', ' ')\n    df_end['fraud'] = df_cleaned['fraud']\n    \n    return df_end\n\n# Execute the preprocessing pipeline\ndf_cleaned = create_cleaned_df(df_train)\nshow_samples(df_train, df_cleaned, 'Before')\nshow_samples(df_train, df_cleaned, 'After')\n\ndf_no_stopwords = remove_stopwords_from_df(df_cleaned)\n\ndf_train_preprocessed = create_final_features(df_no_stopwords)\nprint('Sample (All features):\\n')\nprint(df_train_preprocessed['text'][0])","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:32.924724Z","iopub.status.busy":"2025-05-06T12:29:32.924492Z","iopub.status.idle":"2025-05-06T12:29:34.665817Z","shell.execute_reply":"2025-05-06T12:29:34.665078Z","shell.execute_reply.started":"2025-05-06T12:29:32.924704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1.What the code does so far?**\n\nI've created a comprehensive text preprocessing pipeline for my job posting classification project. My pipeline consists of four main functions working together to transform raw job posting text into clean, consistent features for machine learning. First, I've built a function that displays sample texts before and after processing to verify my transformations. Second, I've created a cleaning function that standardizes binary and categorical features by adding prefixes like \"country_\" or \"telecommuting_yes\" and thoroughly cleans text fields by removing special characters, URLs, numbers, and extra whitespace. Third, I've implemented stopword removal using a combined list from both NLTK and the stop_words package to eliminate common words that don't contribute meaningful information. Finally, I've developed a feature combination function that merges all these cleaned fields into a single comprehensive text representation for each job posting while preserving the fraud classification label.\n\n**2.Why are we doing this?**\n\nI'm performing this extensive preprocessing to transform messy, inconsistent real-world job posting data into a standardized format that machine learning models can effectively analyze. Raw text data contains numerous challenges like inconsistent capitalization, special characters, HTML artifacts, and meaningless stopwords that create noise rather than signal. By cleaning and standardizing this text, I help my classification model focus on meaningful patterns that distinguish fraudulent from legitimate postings. The prefixing approach I've implemented (adding \"country_\" or \"telecommuting_yes\") preserves categorical information while converting it to a format compatible with text-based models. Removing stopwords and standardizing text helps reduce dimensionality and noise while ensuring the model focuses on distinctive terminology that might indicate fraud rather than common words that appear in all postings.\n\n**3.Analysis and insights**\n\nLooking at the samples before and after processing reveals the significant transformation my pipeline achieves. The original texts contain diverse formatting with capitalization, special characters like \"&amp;\", and varying styles. After processing, I've converted everything to lowercase, removed special characters, and standardized spacing. The samples show how elements like \"Recombine is advancing personalized medicine...\" become \"recombine is advancing personalized medicine...\" with consistent formatting. When examining the final combined feature output, I can see how all information from different fields merges into a single comprehensive text representation that preserves both categorical features (like \"telecommuting_no country_us state_ny\") and the important content from description fields. This creates a rich text representation capturing both the structured and unstructured aspects of job postings. The stopword removal process significantly reduces text length by eliminating common words like \"the\" and \"and\" that don't help distinguish between fraudulent and legitimate postings.\n\n**4.Things to keep in mind**\n\nWhile my preprocessing pipeline is thorough, I need to be cautious about potentially removing important signals during cleaning. For instance, some scammers might use specific patterns of special characters or numbers that my cleansing process could eliminate. I should consider preserving certain patterns like email formats or phone number structures that might indicate fraud rather than removing them entirely. I also need to be mindful that combining all features into a single text field loses the relative importance of different sections – perhaps fraudulent posts have distinctive patterns in benefits sections that get diluted when merged with other text. Additionally, my stopword removal might inadvertently eliminate subtle linguistic cues that indicate deception. I should consider experimenting with different levels of preprocessing to find the optimal balance between noise reduction and signal preservation. Finally, I should evaluate whether certain text fields are more predictive than others and potentially weight them differently rather than simply concatenating everything together.","metadata":{}},{"cell_type":"code","source":"def visualize_word_counts(df):\n    \"\"\"\n    Creates visualizations for word count analysis of job postings\n    \n    Args:\n        df: DataFrame with 'text' and 'fraud' columns\n    \n    Returns:\n        DataFrame with word_count column added\n    \"\"\"\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n    \n    # First, calculate the word count for each job posting\n    df = df.copy()\n    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n    \n    # Create a figure with 2 subplots (histogram and boxplot)\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Histogram of word counts by fraud status\n    sns.histplot(\n        data=df, \n        x='word_count',\n        hue='fraud',\n        bins=30,\n        kde=True,\n        ax=axes[0]\n    )\n    axes[0].set_title('Word Count Distribution by Job Status')\n    axes[0].set_xlabel('Number of Words')\n    axes[0].set_ylabel('Count')\n    axes[0].legend(['Real Jobs', 'Fake Jobs'])\n    \n    # Boxplot of word counts by fraud status\n    sns.boxplot(\n        data=df,\n        x='fraud',\n        y='word_count',\n        ax=axes[1]\n    )\n    axes[1].set_title('Word Count Boxplot by Job Status')\n    axes[1].set_xlabel('Job Status (0=Real, 1=Fake)')\n    axes[1].set_ylabel('Number of Words')\n    \n    # Add some descriptive statistics as text\n    for i, fraud_value in enumerate([0, 1]):\n        subset = df[df['fraud'] == fraud_value]['word_count']\n        stats_text = f\"Mean: {subset.mean():.1f}\\nMedian: {subset.median()}\\nStd: {subset.std():.1f}\"\n        axes[1].annotate(\n            stats_text, \n            xy=(i, subset.max()*0.9),\n            ha='center',\n            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n        )\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print some summary statistics\n    print(\"Word count statistics for real vs fake jobs:\")\n    print(df.groupby('fraud')['word_count'].describe())\n    \n    # return df\n\n# Then visualize the word counts\nvisualize_word_counts(df_train_preprocessed)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:34.700633Z","iopub.status.busy":"2025-05-06T12:29:34.700429Z","iopub.status.idle":"2025-05-06T12:29:35.676132Z","shell.execute_reply":"2025-05-06T12:29:35.675561Z","shell.execute_reply.started":"2025-05-06T12:29:34.700619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def u_visualize_word_counts(df):\n    \"\"\"\n    Creates visualizations for word count analysis of job postings\n    \n    Args:\n        df: DataFrame with 'text' and 'fraud' columns\n    \n    Returns:\n        DataFrame with word_count column added\n    \"\"\"\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n    \n    # First, calculate the word count for each job posting\n    df = df.copy()\n    df['word_count'] = df['text'].apply(lambda x: len(set(str(x).lower().split())))\n    \n    # Create a figure with 2 subplots (histogram and boxplot)\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Histogram of word counts by fraud status\n    sns.histplot(\n        data=df, \n        x='word_count',\n        hue='fraud',\n        bins=30,\n        kde=True,\n        ax=axes[0]\n    )\n    axes[0].set_title('Unique Word Count Distribution by Job Status')\n    axes[0].set_xlabel('Number of Words')\n    axes[0].set_ylabel('Count')\n    axes[0].legend(['Real Jobs', 'Fake Jobs'])\n    \n    # Boxplot of word counts by fraud status\n    sns.boxplot(\n        data=df,\n        x='fraud',\n        y='word_count',\n        ax=axes[1]\n    )\n    axes[1].set_title('Unique Word Count Boxplot by Job Status')\n    axes[1].set_xlabel('Job Status (0=Real, 1=Fake)')\n    axes[1].set_ylabel('Number of Words')\n    \n    # Add some descriptive statistics as text\n    for i, fraud_value in enumerate([0, 1]):\n        subset = df[df['fraud'] == fraud_value]['word_count']\n        stats_text = f\"Mean: {subset.mean():.1f}\\nMedian: {subset.median()}\\nStd: {subset.std():.1f}\"\n        axes[1].annotate(\n            stats_text, \n            xy=(i, subset.max()*0.9),\n            ha='center',\n            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n        )\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print some summary statistics\n    print(\"Word count statistics for real vs fake jobs:\")\n    print(df.groupby('fraud')['word_count'].describe())\n    \n    # return df\n\n# Then visualize the word counts\nu_visualize_word_counts(df_train_preprocessed)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:35.677103Z","iopub.status.busy":"2025-05-06T12:29:35.676899Z","iopub.status.idle":"2025-05-06T12:29:37.080543Z","shell.execute_reply":"2025-05-06T12:29:37.079946Z","shell.execute_reply.started":"2025-05-06T12:29:35.677087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Make preprocessing pipeline for validation and test ","metadata":{}},{"cell_type":"code","source":"\n\ndef create_preprocessing_pipeline():\n    \"\"\"\n    Creates a complete preprocessing pipeline for job posting data using FunctionTransformer\n    \n    Returns:\n        sklearn Pipeline that can transform raw dataframes into preprocessed ones\n    \"\"\"\n    # Create transformers from functions using FunctionTransformer\n    text_spec_transformer = FunctionTransformer(add_text_specification_features)\n    location_split_transformer = FunctionTransformer(split_location_features)\n    cleaned_df_transformer = FunctionTransformer(create_cleaned_df)\n    remove_stopwords_df_transformer = FunctionTransformer(remove_stopwords_from_df)\n    final_features_transformer = FunctionTransformer(create_final_features)\n    \n    # Create the pipeline with all transformers in sequence\n    preprocessing_pipeline = make_pipeline(\n        text_spec_transformer,\n        location_split_transformer,\n        cleaned_df_transformer,\n        remove_stopwords_df_transformer,\n        final_features_transformer\n    )\n    \n    return preprocessing_pipeline\n\n# # Even simpler direct usage:\n# def preprocess_data(df):\n#     \"\"\"\n#     Apply all preprocessing steps to a dataframe\n    \n#     Args:\n#         df: Input dataframe with raw job posting data\n        \n#     Returns:\n#         Preprocessed dataframe\n#     \"\"\"\n#     # Apply each function in sequence\n#     df = add_text_specification_features(df)\n#     df = split_location_features(df)\n#     df = create_cleaned_df(df)\n#     df = remove_stopwords_df_transformer(df)\n#     df = create_final_features(df)\n#     return df\n\n# Usage example (with pipeline):\npipeline = create_preprocessing_pipeline()\n# df_train_preprocessed = pipeline.transform(df_train)\ndf_validation_preprocessed = pipeline.transform(df_validation)\ndf_test_preprocessed = pipeline.transform(df_test)\n\n# Usage example (with direct function):\n# df_train_preprocessed = preprocess_data(df_train)\n# df_validation_preprocessed = preprocess_data(df_validation)\n# df_test_preprocessed = preprocess_data(df_test)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:37.081565Z","iopub.status.busy":"2025-05-06T12:29:37.081297Z","iopub.status.idle":"2025-05-06T12:29:39.202778Z","shell.execute_reply":"2025-05-06T12:29:39.202254Z","shell.execute_reply.started":"2025-05-06T12:29:37.081540Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_validation_preprocessed.shape, df_validation_preprocessed.fraud.value_counts(normalize=True))\ndisplay(df_validation_preprocessed.head())\nprint('Sample (All features):\\n')\nprint(df_validation_preprocessed['text'][5897], end='\\n\\n')\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:39.204037Z","iopub.status.busy":"2025-05-06T12:29:39.203819Z","iopub.status.idle":"2025-05-06T12:29:39.213309Z","shell.execute_reply":"2025-05-06T12:29:39.212611Z","shell.execute_reply.started":"2025-05-06T12:29:39.204020Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_test_preprocessed.shape, df_test_preprocessed.fraud.value_counts(normalize=True))\ndisplay(df_test_preprocessed.head())\nprint('Sample (All features):\\n')\nprint(df_test_preprocessed['text'][4708], end='\\n\\n')\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:39.214523Z","iopub.status.busy":"2025-05-06T12:29:39.214154Z","iopub.status.idle":"2025-05-06T12:29:39.232623Z","shell.execute_reply":"2025-05-06T12:29:39.231954Z","shell.execute_reply.started":"2025-05-06T12:29:39.214499Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code does so far?**\n\nI've implemented two key visualization functions that analyze word count distributions in our preprocessed job posting dataset. The first function calculates and visualizes the total word count for each posting, while the second examines unique word counts (counting distinct words only). Both functions create side-by-side histograms and boxplots comparing legitimate versus fraudulent job postings, complete with summary statistics displayed directly on the charts. I've also built a comprehensive preprocessing pipeline using scikit-learn's Pipeline framework that streamlines the entire text transformation process. This pipeline chains together all my previous preprocessing steps: adding text specification features, splitting location data, cleaning and standardizing text, removing stopwords, and combining features into a single text representation. The pipeline automatically applies this entire workflow to my validation and test datasets, ensuring consistent preprocessing across all data splits.\n\n**2. Why are we doing this?**\n\nI'm analyzing word count distributions because they provide critical insights for designing effective text classification models. Understanding both total and unique word counts helps me determine appropriate parameters for TF-IDF vectorization, including max_features, min_df, and max_df settings. The total word count shows overall text length patterns, while unique word counts reveal vocabulary richness and diversity - both potentially strong signals for fraud detection. By comparing these distributions between real and fake jobs, I can identify distinctive patterns that might indicate fraud. Creating a standardized preprocessing pipeline ensures consistency across training, validation, and test datasets, preventing data leakage and making my workflow reproducible. This disciplined approach helps maintain the integrity of my model validation process and will facilitate deployment if the model moves to production.\n\n**3. Analysis and insights**\n\nMy word count analysis reveals striking differences between legitimate and fraudulent job postings. Real job postings contain significantly more words overall, with a mean of 257 words compared to only 199 words for fraudulent posts. The boxplots visually confirm this pattern, showing higher medians and larger interquartile ranges for legitimate postings. When examining unique words, I found that legitimate postings contain a mean of 202 unique words versus 157 for fraudulent postings. This suggests that legitimate employers not only write more but also use more diverse vocabulary when describing positions. The word count histograms show that fraudulent postings are heavily skewed toward shorter lengths, while legitimate postings follow a more normal distribution with some postings reaching considerable length (up to 1450 words). These substantial differences in both overall word count and vocabulary diversity strongly support my hypothesis that text length and complexity are important signals for fraud detection.\n\n**4. Things to keep in mind**\n\nWhile word count statistics provide valuable insights, I need to be cautious about several issues when building my classification model. First, I should be aware that extremely long job postings might contain redundant information, so I'll need appropriate TF-IDF parameters to normalize for document length. The large standard deviation in word counts (around 131 for real jobs, 134 for fake jobs) indicates considerable variation within categories, suggesting I should avoid hard thresholds based solely on word counts. When configuring my TF-IDF vectorizer, I'll need to balance vocabulary size with computational efficiency - the unique word count statistics suggest I should set max_features somewhere above 200 to capture the vocabulary richness of legitimate postings. I should also verify whether certain industries naturally have longer or shorter job descriptions to avoid misclassifying legitimate short postings from certain sectors. Finally, while word count is a useful signal, I'll need to combine it with more sophisticated textual and categorical features for optimal model performance, as sophisticated scammers could potentially pad their fraudulent postings with extra text to evade detection.","metadata":{}},{"cell_type":"markdown","source":"# 4. Sampling and data augmentation methods","metadata":{}},{"cell_type":"code","source":"#https://github.com/jasonwei20/eda_nlp\n\n\ndef create_multiple_sampling_datasets(df_train, df_validation, df_test, text_column='text', label_column='fraud'):\n    \"\"\"\n    Create multiple datasets using different sampling techniques\n    \"\"\"\n\n    \n    # Initialize TF-IDF Vectorizer\n    tfidf = TfidfVectorizer(min_df=2, max_features=5000)\n    \n    # First vectorize original data\n    X_train_tfidf = tfidf.fit_transform(df_train[text_column])\n    X_val_tfidf = tfidf.transform(df_validation[text_column])\n    X_test_tfidf = tfidf.transform(df_test[text_column])\n    \n    # Get target variables\n    y_train = df_train[label_column].values\n    y_val = df_validation[label_column].values\n    y_test = df_test[label_column].values\n    \n    # Initialize samplers (targeting 80:20 ratio = 0.25)\n    oversampler = RandomOverSampler(sampling_strategy=0.25, random_state=42)\n    undersampler = RandomUnderSampler(sampling_strategy=0.25, random_state=42)\n    smote = SMOTE(sampling_strategy=0.25, random_state=42)\n    \n    # Apply sampling methods\n    X_train_over, y_train_over = oversampler.fit_resample(X_train_tfidf, y_train)\n    X_train_under, y_train_under = undersampler.fit_resample(X_train_tfidf, y_train)\n    X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n    \n    # # Apply EDA augmentation to minority class\n    # def get_synonyms(word):\n    #     synonyms = []\n    #     for syn in wordnet.synsets(word):\n    #         for lemma in syn.lemmas():\n    #             if lemma.name() != word:\n    #                 synonyms.append(lemma.name())\n    #     return list(set(synonyms))\n    \n    # def eda_augment(text, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=4):\n    #     words = text.split()\n    #     words = [word for word in words if word]  # Remove empty strings\n    #     num_words = len(words)\n        \n    #     augmented_texts = []\n        \n    #     for _ in range(num_aug):\n    #         current_words = words.copy()\n            \n    #         # 1. Synonym Replacement\n    #         n_sr = max(1, int(alpha_sr * num_words))\n    #         for _ in range(n_sr):\n    #             if not current_words:\n    #                 break\n    #             random_idx = random.randint(0, len(current_words)-1)\n    #             random_word = current_words[random_idx]\n    #             synonyms = get_synonyms(random_word)\n    #             if synonyms:\n    #                 current_words[random_idx] = random.choice(synonyms)\n            \n    #         # 2. Random Insertion\n    #         n_ri = max(1, int(alpha_ri * num_words))\n    #         for _ in range(n_ri):\n    #             if not current_words:\n    #                 break\n    #             random_word = random.choice(current_words)\n    #             synonyms = get_synonyms(random_word)\n    #             if synonyms:\n    #                 random_synonym = random.choice(synonyms)\n    #                 random_idx = random.randint(0, len(current_words))\n    #                 current_words.insert(random_idx, random_synonym)\n            \n    #         # 3. Random Swap\n    #         n_rs = max(1, int(alpha_rs * num_words))\n    #         for _ in range(n_rs):\n    #             if len(current_words) < 2:\n    #                 break\n    #             idx1, idx2 = random.sample(range(len(current_words)), 2)\n    #             current_words[idx1], current_words[idx2] = current_words[idx2], current_words[idx1]\n            \n    #         # 4. Random Deletion\n    #         current_words = [word for word in current_words if random.random() > alpha_rd]\n            \n    #         # Ensure we don't have an empty text\n    #         if current_words:\n    #             augmented_texts.append(' '.join(current_words))\n        \n    #     return augmented_texts\n    \n    # fraud_indices = df_train[df_train[label_column] == 1].index\n    # augmented_texts = []\n    # augmented_labels = []\n    \n    # for idx in fraud_indices:\n    #     aug_texts = eda_augment(\n    #         df_train.loc[idx, text_column],\n    #         alpha_sr=0.1,  # 10% of words for synonym replacement\n    #         alpha_ri=0.1,  # 10% of words for random insertion\n    #         alpha_rs=0.1,  # 10% of words for random swap\n    #         alpha_rd=0.1,  # 10% of words for random deletion\n    #         num_aug=4      # Generate 4 augmented versions\n    #     )\n    #     augmented_texts.extend(aug_texts)\n    #     augmented_labels.extend([1] * len(aug_texts))\n    \n    # # Create augmented dataset\n    # df_train_aug = pd.DataFrame({\n    #     text_column: list(df_train[text_column]) + augmented_texts,\n    #     label_column: list(df_train[label_column]) + augmented_labels\n    # })\n    \n    # # Vectorize augmented dataset\n    # X_train_aug = tfidf.transform(df_train_aug[text_column])\n    # y_train_aug = df_train_aug[label_column].values\n    \n    # Store all datasets in dictionary - only return X and y as tuples\n    datasets = {\n        'original': {\n            'train': (X_train_tfidf, y_train),\n            'val': (X_val_tfidf, y_val),\n            'test': (X_test_tfidf, y_test)\n        },\n        'oversampled': {\n            'train': (X_train_over, y_train_over),\n            'val': (X_val_tfidf, y_val),\n            'test': (X_test_tfidf, y_test)\n        },\n        'undersampled': {\n            'train': (X_train_under, y_train_under),\n            'val': (X_val_tfidf, y_val),\n            'test': (X_test_tfidf, y_test)\n        },\n        'smote': {\n            'train': (X_train_smote, y_train_smote),\n            'val': (X_val_tfidf, y_val),\n            'test': (X_test_tfidf, y_test)\n        }\n        # 'eda_augmented': {\n        #     'train': (X_train_aug, y_train_aug),\n        #     'val': (X_val_tfidf, y_val),\n        #     'test': (X_test_tfidf, y_test)\n        # }\n    }\n    \n    # Create dataframe with metrics\n    summary_data = []\n    for name, data in datasets.items():\n        X_shape = data['train'][0].shape\n        y_counts = pd.Series(data['train'][1]).value_counts()\n        y_pct = pd.Series(data['train'][1]).value_counts(normalize=True) * 100\n        \n        if 1 in y_counts.index and 0 in y_counts.index:\n            fraud_count = y_counts[1]\n            non_fraud_count = y_counts[0]\n            fraud_pct = y_pct[1]\n            non_fraud_pct = y_pct[0]\n        else:\n            fraud_count = y_counts.get(1, 0)\n            non_fraud_count = y_counts.get(0, 0)\n            fraud_pct = y_pct.get(1, 0)\n            non_fraud_pct = y_pct.get(0, 0)\n            \n        summary_data.append({\n            'Method': name,\n            'Total Samples': X_shape[0],\n            'Features': X_shape[1],\n            'Fraud Count': fraud_count,\n            'Non-Fraud Count': non_fraud_count,\n            'Fraud %': fraud_pct,\n            'Non-Fraud %': non_fraud_pct\n        })\n    \n    df_summary = pd.DataFrame(summary_data)\n    \n    # Store original and augmented dataframes separately\n    dataframes = {\n        'original': df_train,\n        'validation': df_validation,\n        'test': df_test,\n        # 'augmented': df_train_aug\n    }\n    \n    return datasets, df_summary, tfidf, dataframes\n\n\n# Step 1: Create the datasets\ndatasets, df_summary, tfidf, dataframes = create_multiple_sampling_datasets(\n    df_train_preprocessed, \n    df_validation_preprocessed, \n    df_test_preprocessed\n)\n\n# Print the summary table\nprint(\"Dataset Summary:\")\nprint(df_summary)\n\nall_data = {\n    'datasets': datasets,\n    'df_summary': df_summary,\n    'tfidf': tfidf,\n    'dataframes': dataframes\n}\n\n# Save final model\njoblib.dump(all_data, 'all_data.pkl')\n\nprint(\"All data saved to pickle files successfully!\")","metadata":{"execution":{"iopub.execute_input":"2025-05-06T15:03:53.544014Z","iopub.status.busy":"2025-05-06T15:03:53.543708Z","iopub.status.idle":"2025-05-06T15:03:58.185320Z","shell.execute_reply":"2025-05-06T15:03:58.184366Z","shell.execute_reply.started":"2025-05-06T15:03:53.543994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets['original']","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:43.136512Z","iopub.status.busy":"2025-05-06T12:29:43.136306Z","iopub.status.idle":"2025-05-06T12:29:43.142248Z","shell.execute_reply":"2025-05-06T12:29:43.141475Z","shell.execute_reply.started":"2025-05-06T12:29:43.136496Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code does so far?**\n\nI've implemented a comprehensive function to tackle the class imbalance problem in my job posting fraud detection project. The function creates multiple balanced datasets using different sampling techniques while maintaining consistent preprocessing and feature extraction. First, I vectorize my preprocessed text data using TF-IDF with 5000 max features to capture the most important terms across all job postings. Then I apply three different resampling strategies to address the severe class imbalance: RandomOverSampler (which duplicates minority class samples), RandomUnderSampler (which reduces majority class samples), and SMOTE (which generates synthetic minority samples). For each technique, I maintain the same validation and test sets while only transforming the training data to achieve a target 80:20 class ratio (non-fraudulent to fraudulent). The function generates a comprehensive summary dataframe showing the composition of each dataset variation, clearly demonstrating how each technique addresses the imbalance. I've also included code to save all these datasets, the TF-IDF vectorizer, and summary statistics to a pickle file for later use.\n\n**2. Why are we doing this?**\n\nI'm creating these balanced datasets because the original data has an extreme class imbalance – only 4.85% of job postings are fraudulent compared to 95.15% legitimate ones. This imbalance can severely impact model performance, causing classifiers to favor the majority class and potentially miss fraudulent postings, which are precisely what we want to detect. I chose TF-IDF vectorization rather than word2vec or other embeddings because of its simplicity and effectiveness for this particular problem. Since my preprocessing pipeline created many categorical features with prefix encoding (like \"has_telecommuting_no\"), TF-IDF is well-suited to capture both these categorical indicators and textual patterns in a unified representation. Word embeddings would add complexity without necessarily improving performance for these mixed feature types. I selected 5000 max features for my TF-IDF vectorizer based on my earlier word count analysis, which showed legitimate postings have around 202 unique words on average, while fraudulent ones have about 157. I deliberately chose an 80:20 ratio of majority to minority classes rather than a 50:50 split because it better reflects real-world distributions while still addressing the extreme imbalance. A completely balanced 50:50 approach might lead to too many false positives in production, while the 80:20 ratio offers a good compromise between fraud detection and minimizing false alarms.\n\n**3. Analysis and insights**\n\nLooking at the summary table, I can see how dramatically each sampling technique transforms the training data. The original dataset contains just 656 fraudulent examples among 13,517 total samples (4.85% fraud). After oversampling, the number of fraudulent examples increases substantially to 3,215 while keeping all 12,861 legitimate samples, creating a much more balanced 20:80 ratio. Interestingly, SMOTE produces identical counts to oversampling, but with synthetic examples rather than duplicated ones. The undersampling approach takes a different tack, maintaining all 656 fraudulent examples but drastically reducing legitimate samples to just 2,624, resulting in a much smaller overall dataset (3,280 samples) but with the same target 20:80 ratio. This variety of approaches gives me flexibility to evaluate which balancing technique works best with different classification algorithms. Models like random forests that handle imbalance better might work well with the original data, while algorithms more sensitive to class imbalance might perform better with one of the balanced datasets.\n\n**4. Things to keep in mind**\n\nWhile addressing class imbalance is critical, I need to be cautious about several issues when evaluating models trained on these balanced datasets. First, oversampling by duplicating minority samples might lead to overfitting, as the model sees the same rare examples multiple times. SMOTE creates synthetic examples that might not represent real-world fraudulent postings accurately, potentially introducing noise. Undersampling discards a significant portion of my legitimate data, which could eliminate valuable information. I also need to remember that my validation and test sets maintain the original class distribution, which is appropriate for evaluating real-world performance. When comparing models, I should focus on metrics like precision, recall, and F1-score rather than accuracy, since accuracy can be misleading with imbalanced classes. I originally explored text augmentation techniques from the EDA-NLP repository (Synonym Replacement, Random Insertion, Random Swap, Random Deletion) but found they didn't significantly improve performance while adding complexity, so I've opted for simpler approaches. Finally, I should consider ensemble methods that could potentially leverage the strengths of models trained on different sampling techniques to achieve superior fraud detection performance.","metadata":{}},{"cell_type":"markdown","source":"# 5. Modelling ( Ensemble and traditional)","metadata":{}},{"cell_type":"code","source":"def train_evaluate_models(datasets):\n    \"\"\"\n    Train and evaluate multiple models on different datasets\n    \"\"\"\n\n    \n    def get_metrics(y_true, y_pred, y_pred_proba=None):\n        \"\"\"Calculate all required metrics\"\"\"\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n        \n        precision = precision_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred)\n        \n        # Calculate AUC-ROC if probabilities are provided\n        auc_roc = roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else None\n        \n        return tp, tn, fp, fn, precision, recall, f1, auc_roc\n        \n    # Initialize models\n    models = {\n        'LogisticRegression': LogisticRegression(random_state=42),\n        'RandomForest': RandomForestClassifier(random_state=42),\n        'XGBoost': xgb.XGBClassifier(random_state=42),\n        'LightGBM': lgb.LGBMClassifier(random_state=42),\n        # 'CatBoost': CatBoostClassifier(random_state=42, verbose=False)\n    }\n    \n    # Initialize results list\n    results = []\n    \n    # Train and evaluate each model on each dataset\n    for model_name, model in models.items():\n        print(f\"\\nTraining {model_name}...\")\n        \n        for data_type in datasets.keys():\n            print(f\"\\nEvaluating on {data_type} dataset:\")\n            \n            # Get data\n            X_train, y_train = datasets[data_type]['train']\n            X_val, y_val = datasets[data_type]['val']\n            X_test, y_test = datasets[data_type]['test']\n            \n            # Train model\n            model.fit(X_train, y_train)\n            \n            # Get predictions for each dataset\n            datasets_eval = {\n                'train': (X_train, y_train),\n                'validation': (X_val, y_val),\n                'test': (X_test, y_test)\n            }\n            \n            for dataset_name, (X, y) in datasets_eval.items():\n                y_pred = model.predict(X)\n                y_pred_proba = model.predict_proba(X)[:, 1]\n                \n                # Calculate metrics\n                tp, tn, fp, fn, precision, recall, f1, auc_roc = get_metrics(y, y_pred, y_pred_proba)\n                \n                # Add results to list\n                results.append({\n                    'Model': model_name,\n                    'Data_Type': data_type,\n                    'Dataset': dataset_name,\n                    'TP': tp,\n                    'TN': tn,\n                    'FP': fp,\n                    'FN': fn,\n                    'Precision': precision,\n                    'Recall': recall,\n                    'F1': f1,\n                    'AUC_ROC': auc_roc\n                })\n                \n                # Print current results\n                print(f\"\\n{dataset_name} set metrics:\")\n                print(f\"Precision: {precision:.4f}\")\n                print(f\"Recall: {recall:.4f}\")\n                print(f\"F1: {f1:.4f}\")\n                print(f\"AUC-ROC: {auc_roc:.4f}\")\n    \n    # Create metrics DataFrame\n    metrics_df = pd.DataFrame(results)\n    \n    # Display grouped results\n    print(\"\\nMetrics Summary Grouped by Model and Dataset:\")\n    grouped_metrics = metrics_df.groupby(['Model', 'Data_Type', 'Dataset'])[\n        ['Precision', 'Recall', 'F1', 'AUC_ROC']\n    ].mean()\n    \n    # Find best model and dataset combination based on F1 score on test set\n    best_results = metrics_df[metrics_df['Dataset'] == 'test'].sort_values('F1', ascending=False)\n    print(\"\\nTop 5 Model-Dataset Combinations (based on Test F1 score):\")\n    top5 = best_results[['Model', 'Data_Type', 'F1', 'AUC_ROC']].head(5)\n    \n    return metrics_df, grouped_metrics, top5","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:43.143405Z","iopub.status.busy":"2025-05-06T12:29:43.143098Z","iopub.status.idle":"2025-05-06T12:29:43.160322Z","shell.execute_reply":"2025-05-06T12:29:43.159786Z","shell.execute_reply.started":"2025-05-06T12:29:43.143377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Train and evaluate models\nmetrics_df, grouped_metrics, val_top5 = train_evaluate_models(datasets)\nprint(\"\\nGrouped metrics by model and dataset:\")\nprint(grouped_metrics)\n\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:29:43.161126Z","iopub.status.busy":"2025-05-06T12:29:43.160968Z","iopub.status.idle":"2025-05-06T12:32:56.234445Z","shell.execute_reply":"2025-05-06T12:32:56.233624Z","shell.execute_reply.started":"2025-05-06T12:29:43.161114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_results = metrics_df[metrics_df['Dataset'] == 'validation'].sort_values('F1', ascending=False)\nprint(\"\\nTop 5 Model-Dataset Combinations (based on validation F1 score):\")\nbest_results[['Model', 'Data_Type', 'F1', 'AUC_ROC']].head(5)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:32:56.236633Z","iopub.status.busy":"2025-05-06T12:32:56.235313Z","iopub.status.idle":"2025-05-06T12:32:56.245779Z","shell.execute_reply":"2025-05-06T12:32:56.245191Z","shell.execute_reply.started":"2025-05-06T12:32:56.236608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  6. Hyperparameter tunning","metadata":{"execution":{"iopub.execute_input":"2025-04-29T12:33:30.402731Z","iopub.status.busy":"2025-04-29T12:33:30.402425Z","iopub.status.idle":"2025-04-29T12:33:30.406725Z","shell.execute_reply":"2025-04-29T12:33:30.405856Z","shell.execute_reply.started":"2025-04-29T12:33:30.402711Z"}}},{"cell_type":"code","source":"# Step 1: Define hyperparameter search space\nspace = {\n    'C': hp.loguniform('C', np.log(0.01), np.log(100)),\n    'solver': hp.choice('solver', ['liblinear', 'lbfgs', 'saga']),\n    'penalty': hp.choice('penalty', ['l1', 'l2']),\n    'max_iter': hp.quniform('max_iter', 100, 1000, 100),\n    'class_weight': hp.choice('class_weight', [None, 'balanced'])\n}\n\n# Define choices for easy lookup\nsolver_choices = ['liblinear', 'lbfgs', 'saga']\npenalty_choices = ['l1', 'l2']\nclass_weight_choices = [None, 'balanced']\n\n# Step 2: Get SMOTE train data and original validation data\nX_train_smote, y_train_smote = datasets['smote']['train']\nX_val_original, y_val_original = datasets['original']['val']\n\n# Step 3: Define objective function for hyperopt\ndef objective(params):\n    # Hyperopt returns indices for hp.choice, so we need to map them to actual values\n    solver = params['solver']  # Already should be the string value\n    penalty = params['penalty']  # Already should be the string value\n    class_weight = params['class_weight']  # Already should be the actual value\n    \n    # Adjust solver-penalty compatibility\n    if solver == 'lbfgs' and penalty == 'l1':\n        solver = 'saga'\n    \n    # Convert parameters\n    model_params = {\n        'C': params['C'],\n        'solver': solver,\n        'penalty': penalty,\n        'max_iter': int(params['max_iter']),\n        'class_weight': class_weight,\n        'random_state': 42,\n        'n_jobs': -1\n    }\n    \n    # Train on SMOTE data, validate on original data\n    model = LogisticRegression(**model_params)\n    model.fit(X_train_smote, y_train_smote)\n    \n    # Predict on original validation\n    y_pred = model.predict(X_val_original)\n    val_f1 = f1_score(y_val_original, y_pred)\n    \n    # Return negative F1 since hyperopt minimizes\n    return {'loss': -val_f1, 'status': STATUS_OK}\n\n# Step 4: Run hyperopt\nprint(\"Running hyperparameter optimization...\")\ntrials = Trials()\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Run hyperopt\nbest = fmin(\n    objective, \n    space, \n    algo=tpe.suggest, \n    max_evals=50, \n    trials=trials,\n    verbose=1\n)\n\n# Step 5: Get best parameters\n# Hyperopt returns indices for hp.choice, so we need to map them to actual values\nsolver = solver_choices[int(best['solver'])]\npenalty = penalty_choices[int(best['penalty'])]\nclass_weight = class_weight_choices[int(best['class_weight'])]\n\nbest_params = {\n    'C': best['C'],\n    'solver': solver,\n    'penalty': penalty,\n    'max_iter': int(best['max_iter']),\n    'class_weight': class_weight,\n    'random_state': 42,\n    'n_jobs': -1\n}\n\n# Adjust solver-penalty compatibility for final model\nif best_params['solver'] == 'lbfgs' and best_params['penalty'] == 'l1':\n    best_params['solver'] = 'saga'\n\nprint(\"Best Parameters:\")\nfor param, value in best_params.items():\n    print(f\"{param}: {value}\")\n\n# Save best parameters\njoblib.dump(best_params, 'best_params.pkl')\n\n# Step 6: Combine original train and validation datasets\nX_train_original, y_train_original = datasets['original']['train']\nX_val_original, y_val_original = datasets['original']['val']\n\n# Combine the sparse matrices and arrays\nX_combined = vstack([X_train_original, X_val_original])\ny_combined = np.concatenate([y_train_original, y_val_original])\n\nprint(f\"Combined dataset shape: {X_combined.shape}\")\nprint(f\"Combined label distribution: {np.bincount(y_combined)}\")\n\n# Step 7: Apply SMOTE to combined dataset\nsmote = SMOTE(sampling_strategy=0.25, random_state=42)\nX_combined_smote, y_combined_smote = smote.fit_resample(X_combined, y_combined)\n\nprint(f\"After SMOTE - Shape: {X_combined_smote.shape}\")\nprint(f\"After SMOTE - Label distribution: {np.bincount(y_combined_smote)}\")\n\n# Step 8: Train final model with best parameters\nfinal_model = LogisticRegression(**best_params)\nfinal_model.fit(X_combined_smote, y_combined_smote)\n\n# Save final model\njoblib.dump(final_model, 'final_model.pkl')\n\n# Step 9: Test on original test set\nX_test_original, y_test_original = datasets['original']['test']\n\n# Make predictions\ny_pred = final_model.predict(X_test_original)\ny_pred_proba = final_model.predict_proba(X_test_original)[:, 1]\n\n# Step 10: Calculate metrics\ntn, fp, fn, tp = confusion_matrix(y_test_original, y_pred).ravel()\nprecision = precision_score(y_test_original, y_pred)\nrecall = recall_score(y_test_original, y_pred)\nf1 = f1_score(y_test_original, y_pred)\nauc_roc = roc_auc_score(y_test_original, y_pred_proba)\n\n# Print final test results\nprint(\"\\nFinal Test Results:\")\nprint(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1: {f1:.4f}\")\nprint(f\"AUC-ROC: {auc_roc:.4f}\")\n\n# Save test results\ntest_results = {\n    'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n    'Precision': precision, 'Recall': recall, \n    'F1': f1, 'AUC_ROC': auc_roc\n}\n\npd.DataFrame([test_results]).to_csv('test_results.csv', index=False)\nprint(\"\\nTest results saved to test_results.csv\")\nprint(\"Best parameters saved to best_params.pkl\")\nprint(\"Final model saved to final_model.pkl\")","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:32:56.262930Z","iopub.status.busy":"2025-05-06T12:32:56.262686Z","iopub.status.idle":"2025-05-06T12:47:33.696500Z","shell.execute_reply":"2025-05-06T12:47:33.695587Z","shell.execute_reply.started":"2025-05-06T12:32:56.262912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **1. What the code does so far?**\n\nI've implemented a comprehensive model evaluation framework that trains and compares multiple classification algorithms across different sampling techniques. My function trains four different models (Logistic Regression, Random Forest, XGBoost, and LightGBM) on each of our balanced datasets (original, oversampled, undersampled, and SMOTE). For every model-dataset combination, I calculate and report crucial performance metrics including precision, recall, F1 score, and AUC-ROC on the training, validation, and test sets. I've also created a comparative analysis that identifies the top-performing combinations based on test set F1 scores. The code automatically generates a structured metrics dataframe and provides grouped summaries to facilitate easy comparison across different dimensions. This allows me to systematically identify which model and sampling technique combination yields the best fraud detection performance.\n\n **2. Why are we doing this?**\n\nI'm conducting this extensive model comparison to find the optimal approach for detecting fraudulent job postings in an imbalanced dataset. Given the serious consequences of both false positives (flagging legitimate jobs as fraud) and false negatives (missing actual scams), I need robust evaluation metrics that account for class imbalance. That's why I've chosen F1 score and AUC-ROC as my primary metrics. F1 score provides a balanced measure combining precision and recall, which is critical in fraud detection where both accuracy in flagging fraud (precision) and catching all instances of fraud (recall) matter equally. AUC-ROC measures the model's ability to discriminate between classes regardless of the chosen threshold, making it particularly valuable for imbalanced datasets. By testing multiple models across different sampling techniques, I can determine which combination best handles the inherent challenges of fraud detection while generalizing well to unseen data.\n\n**3. Analysis and insights**\n\nMy evaluation reveals fascinating patterns across model and sampling technique combinations. LightGBM trained on SMOTE-enhanced data emerges as the top performer on the test set with an impressive F1 score of 0.904, striking an excellent balance between precision (0.976) and recall (0.842). Looking at validation set performance, XGBoost with SMOTE leads with an F1 score of 0.853, followed closely by Logistic Regression with SMOTE at 0.852. What's particularly interesting is how similarly LightGBM, Logistic Regression, and XGBoost perform when trained on SMOTE data, with all achieving validation F1 scores above 0.85 and test F1 scores above 0.84. The SMOTE sampling approach consistently outperforms other techniques across almost all models, suggesting that creating synthetic fraud examples is more effective than simple oversampling or undersampling. I've also noticed these models show less overfitting on SMOTE data compared to other sampling approaches, with smaller gaps between training and validation metrics.\n\n **4. Things to keep in mind**\n\nGiven the similar performance of top models on SMOTE data, I would select Logistic Regression for production deployment if interpretability and training speed are priorities. Logistic Regression offers faster training times that would benefit hyperparameter tuning iterations and provides straightforward feature importance scores that could help explain why specific job postings get flagged as fraudulent. If I needed better performance without sacrificing too much speed, LightGBM would be my choice as it's significantly faster than XGBoost during hyperparameter tuning while delivering comparable results. XGBoost would be my preference only if maximum performance is the absolute priority regardless of training time. I should also remember that all models show perfect training set performance, indicating some risk of overfitting despite the good validation and test scores. For future work, I might explore stacking these complementary models to potentially improve performance further. Finally, while my models perform well on this dataset, fraudsters constantly evolve their tactics, so I should plan to regularly retrain models on fresh data to maintain detection accuracy.","metadata":{}},{"cell_type":"markdown","source":"# 7. Interpretating model","metadata":{}},{"cell_type":"code","source":"\n# Create wrapper function for the hypertuned model\ndef predict_proba_text(text_array):\n    \"\"\"Simple wrapper for model prediction\"\"\"\n    text_features = tfidf.transform(text_array)\n    return final_model.predict_proba(text_features)\n\n# Initialize LIME explainer\nexplainer = LimeTextExplainer(class_names=['Non-Fraud', 'Fraud'])\n\n# Get one fraud and one non-fraud example from test set\ntest_df = dataframes['test']\nfraud_idx = test_df[test_df['fraud'] == 1].index[1]\nnon_fraud_idx = test_df[test_df['fraud'] == 0].index[0]\n\n# Explain non-fraud example\nnon_fraud_text = test_df.loc[non_fraud_idx, 'text']\nnon_fraud_explanation = explainer.explain_instance(\n    non_fraud_text, \n    predict_proba_text,\n    num_features=10\n)\nnon_fraud_explanation.show_in_notebook(text=True)\n\n# Explain fraud example  \nfraud_text = test_df.loc[fraud_idx, 'text']\nfraud_explanation = explainer.explain_instance(\n    fraud_text, \n    predict_proba_text,\n    num_features=10\n)\nfraud_explanation.show_in_notebook(text=True)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T12:47:33.704571Z","iopub.status.busy":"2025-05-06T12:47:33.704415Z","iopub.status.idle":"2025-05-06T12:47:35.389483Z","shell.execute_reply":"2025-05-06T12:47:35.388623Z","shell.execute_reply.started":"2025-05-06T12:47:33.704558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code does so far?**\n\nI've implemented LIME (Local Interpretable Model-agnostic Explanations) to make my fraud detection model more transparent and interpretable. This technique helps me understand exactly why my model classifies certain job postings as fraudulent or legitimate. I created a simple wrapper function that takes raw text, processes it through my TF-IDF vectorizer, and feeds it into my final classification model. Then I set up a LIME explainer specifically for text data with appropriate class labels. To demonstrate how this works, I selected two contrasting examples from my test set – one legitimate job posting and one fraudulent one – and asked LIME to generate explanations for each prediction. For each job posting, LIME highlights the top 10 features influencing the classification decision, showing which specific words or characteristics push the model toward a \"fraud\" or \"non-fraud\" prediction, along with the strength of each feature's contribution.\n\n**2. Why are we doing this?**\n\nI'm using LIME because machine learning models, particularly those working with text data, often function as \"black boxes\" where the reasoning behind predictions isn't obvious. In a high-stakes application like fraud detection, it's insufficient to just know that a job posting is classified as fraudulent – I need to understand why. This explainability is crucial for several reasons: it builds trust in the model by showing the rationale behind decisions, helps identify potential biases or weaknesses in the model's reasoning, and provides actionable insights for both job seekers and platform administrators. For example, understanding that the absence of a company logo strongly contributes to fraud predictions can help legitimate companies improve their postings, while platform moderators can use these insights to focus their investigation on the most suspicious elements. Ultimately, LIME helps bridge the gap between complex model behavior and human understanding.\n\n**3. Analysis and insights**\n\nLooking at my LIME explanations reveals fascinating patterns in how my model identifies fraudulent postings. For the legitimate job (a Python engineer position), the model is extremely confident (100% probability of being non-fraudulent), but interestingly, no single feature strongly drives this classification. This suggests the model recognizes this as legitimate based on the holistic pattern rather than specific keywords. In contrast, for the fraudulent posting (a home-based data entry clerk position), several features stand out as red flags. The word \"entry\" contributes most strongly to the fraud prediction (0.17 weight), likely because scammers frequently target entry-level workers. The absence of a company logo (\"company_logo_no\" at 0.06) and company profile (\"company_profile_no\" at 0.04) are significant indicators of fraud, confirming my earlier binary feature analysis. Words like \"earn,\" \"motivated,\" and \"need\" also contribute to the fraud prediction, suggesting the model has learned to identify language patterns typical of scam opportunities that promise easy earnings and target motivated individuals who need money.\n\n**4. Things to keep in mind**\n\nWhile LIME provides valuable insights, I need to remember several important considerations when interpreting these explanations. First, LIME approximates the model's behavior locally around specific examples, so patterns that appear in one fraudulent posting might not generalize to all fraud cases. Second, some legitimate postings might contain words typically associated with fraud (like \"entry-level\" positions at real companies), so these explanations should guide but not replace human judgment. Third, as scammers adapt their tactics, these indicators will likely evolve over time, requiring periodic retraining and re-examination of key features. I should also be careful about potential biases – if certain industries or regions legitimately use terminology that overlaps with fraud indicators, they might face higher false positive rates. Finally, I could enhance this analysis by examining more examples across different job categories and industries to build a more comprehensive understanding of fraud indicators in different contexts. This would help create a more nuanced approach to fraud detection that accounts for legitimate variations in job posting styles.","metadata":{}},{"cell_type":"code","source":"\n# Use training data for SHAP analysis\n# Convert sparse matrix to dense if needed\nif hasattr(X_train_smote, \"toarray\"):\n    X_train_dense = X_train_smote.toarray()\nelse:\n    X_train_dense = X_train_smote\n\n# Create background data for KernelExplainer (use a smaller sample for computational efficiency)\nX_background = shap.sample(X_train_dense, 100)  # Sample 100 instances for background\n\n# Select appropriate explainer based on model type\nprint(f\"Model type: {type(final_model).__name__}\")\nprint(\"Creating SHAP explainer...\")\n\n# Get feature names if available\nif 'tfidf' in globals() and hasattr(tfidf, 'get_feature_names_out'):\n    feature_names = tfidf.get_feature_names_out()\nelse:\n    feature_names = None\n\n# Tree-based models\nif isinstance(final_model, (RandomForestClassifier, GradientBoostingClassifier)) or \\\n   'RandomForest' in str(type(final_model)) or \\\n   'GradientBoosting' in str(type(final_model)) or \\\n   isinstance(final_model, (xgb.XGBModel, lgb.LGBMModel)) or \\\n   'XGB' in str(type(final_model)) or \\\n   'LGBM' in str(type(final_model)):\n    print(\"Using TreeExplainer for tree-based model...\")\n    explainer = shap.TreeExplainer(final_model)\n    shap_values = explainer.shap_values(X_train_dense)\n    \n    # For multi-class models, the shap_values will be a list (one per class)\n    # We'll check if we need to select a specific class for binary classification\n    if isinstance(shap_values, list) and len(shap_values) == 2:\n        # For binary classification, focus on class 1 (positive class)\n        print(\"Binary classification detected, using positive class for visualization\")\n        shap_values = shap_values[1]\n    \n# Linear models (like logistic regression)\nelif isinstance(final_model, LogisticRegression) or 'Logistic' in str(type(final_model)):\n    print(\"Using LinearExplainer for logistic regression model...\")\n    # For logistic regression, we use the LinearExplainer\n    explainer = shap.LinearExplainer(final_model, X_background)\n    shap_values = explainer.shap_values(X_train_dense)\n    \n    # For multi-class, select the positive class for binary classification\n    if isinstance(shap_values, list) and len(shap_values) == 2:\n        shap_values = shap_values[1]\n\n# Other model types - use KernelExplainer as a fallback\nelse:\n    print(\"Using KernelExplainer as fallback for this model type...\")\n    # Define a prediction function based on the model's prediction method\n    def model_predict(X):\n        # For binary classification with predict_proba\n        if hasattr(final_model, 'predict_proba'):\n            return final_model.predict_proba(X)[:,1]\n        # For regression models or other cases\n        else:\n            return final_model.predict(X)\n    \n    # Create a KernelExplainer\n    explainer = shap.KernelExplainer(model_predict, X_background)\n    shap_values = explainer.shap_values(X_train_dense)\n\n# Create the summary plot\nprint(\"Creating SHAP summary plot...\")\nif feature_names is not None:\n    shap.summary_plot(shap_values, X_train_dense, feature_names=feature_names)\nelse:\n    shap.summary_plot(shap_values, X_train_dense)\n\n# Just display the plot, don't save it\nplt.tight_layout()\nplt.show()\n\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T13:25:47.497483Z","iopub.status.busy":"2025-05-06T13:25:47.497132Z","iopub.status.idle":"2025-05-06T13:25:51.502055Z","shell.execute_reply":"2025-05-06T13:25:51.501249Z","shell.execute_reply.started":"2025-05-06T13:25:47.497463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Convert to dense array if needed\nif hasattr(X_test_original, \"toarray\"):\n    X_test_dense = X_test_original.toarray()\nelse:\n    X_test_dense = X_test_original\n\n# Create background data for KernelExplainer (use a smaller sample for computational efficiency)\nX_background = shap.sample(X_test_dense, min(100, X_test_dense.shape[0]))  # Sample up to 100 instances\n\n# Select appropriate explainer based on model type\nprint(f\"Model type: {type(final_model).__name__}\")\nprint(\"Creating SHAP explainer...\")\n\n# Get feature names if available\nif 'tfidf' in globals() and hasattr(tfidf, 'get_feature_names_out'):\n    feature_names = tfidf.get_feature_names_out()\nelse:\n    feature_names = None\n\n# Tree-based models\nif isinstance(final_model, (RandomForestClassifier, GradientBoostingClassifier)) or \\\n   'RandomForest' in str(type(final_model)) or \\\n   'GradientBoosting' in str(type(final_model)) or \\\n   isinstance(final_model, (xgb.XGBModel, lgb.LGBMModel)) or \\\n   'XGB' in str(type(final_model)) or \\\n   'LGBM' in str(type(final_model)):\n    print(\"Using TreeExplainer for tree-based model...\")\n    explainer = shap.TreeExplainer(final_model)\n    shap_values = explainer.shap_values(X_test_dense)\n    \n    # For multi-class models, the shap_values will be a list (one per class)\n    # We'll check if we need to select a specific class for binary classification\n    if isinstance(shap_values, list) and len(shap_values) == 2:\n        print(\"Binary classification detected, using positive class for visualization\")\n        class_shap_values = shap_values[1]  # Positive class SHAP values\n    else:\n        class_shap_values = shap_values  # Keep as is for multiclass or if not a list\n    \n# Linear models (like logistic regression)\nelif isinstance(final_model, LogisticRegression) or 'Logistic' in str(type(final_model)):\n    print(\"Using LinearExplainer for logistic regression model...\")\n    # For logistic regression, we use the LinearExplainer\n    explainer = shap.LinearExplainer(final_model, X_background)\n    shap_values = explainer.shap_values(X_test_dense)\n    \n    # For multi-class, select the positive class for binary classification\n    if isinstance(shap_values, list) and len(shap_values) == 2:\n        class_shap_values = shap_values[1]  # Positive class SHAP values\n    else:\n        class_shap_values = shap_values\n\n# Other model types - use KernelExplainer as a fallback\nelse:\n    print(\"Using KernelExplainer as fallback for this model type...\")\n    # Define a prediction function based on the model's prediction method\n    def model_predict(X):\n        # For binary classification with predict_proba\n        if hasattr(final_model, 'predict_proba'):\n            return final_model.predict_proba(X)[:,1]\n        # For regression models or other cases\n        else:\n            return final_model.predict(X)\n    \n    # Create a KernelExplainer\n    explainer = shap.KernelExplainer(model_predict, X_background)\n    shap_values = explainer.shap_values(X_test_dense)\n    class_shap_values = shap_values  # KernelExplainer typically returns values for the function output\n\n# Print the shape to verify\nprint(\"SHAP values shape:\", \n      class_shap_values.shape if hasattr(class_shap_values, 'shape') \n      else [sv.shape if hasattr(sv, 'shape') else \"Unknown shape\" for sv in class_shap_values] \n      if isinstance(class_shap_values, list) else \"Unknown format\")\n\n# Create the summary plot\nprint(\"Creating SHAP summary plot...\")\nif feature_names is not None:\n    shap.summary_plot(class_shap_values, X_test_dense, feature_names=feature_names)\nelse:\n    shap.summary_plot(class_shap_values, X_test_dense)\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n# Optional: Print top N most important features\ndef print_top_features(shap_values, feature_names=None, top_n=10):\n    # Calculate the mean absolute SHAP value for each feature\n    if isinstance(shap_values, list):\n        # For multi-class, take the average over all classes\n        mean_abs_shap = np.mean([np.abs(sv).mean(0) for sv in shap_values], axis=0)\n    else:\n        mean_abs_shap = np.abs(shap_values).mean(0)\n    \n    # Get the indices of the top features\n    top_indices = np.argsort(mean_abs_shap)[-top_n:][::-1]\n    \n    print(f\"\\nTop {top_n} most important features:\")\n    for i, idx in enumerate(top_indices):\n        if feature_names is not None:\n            print(f\"{i+1}. {feature_names[idx]}: {mean_abs_shap[idx]:.4f}\")\n        else:\n            print(f\"{i+1}. Feature {idx}: {mean_abs_shap[idx]:.4f}\")\n\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T13:27:14.669438Z","iopub.status.busy":"2025-05-06T13:27:14.668907Z","iopub.status.idle":"2025-05-06T13:27:15.573350Z","shell.execute_reply":"2025-05-06T13:27:15.572693Z","shell.execute_reply.started":"2025-05-06T13:27:14.669414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code does so far?**\n\nI've implemented SHAP (SHapley Additive exPlanations) analysis to provide deeper global interpretability for my job posting fraud detection model. First, I created appropriate SHAP explainers based on my model type - since I'm using a logistic regression model, my code automatically selects LinearExplainer. The code intelligently handles different model types by checking the model class and selecting the appropriate explainer (TreeExplainer for random forests or gradient boosted models, LinearExplainer for linear models, or KernelExplainer as a fallback). It then generates SHAP values for my test data points, showing how each feature contributes to pushing the prediction toward or away from the fraud class. Finally, I create summary plots that visualize these feature contributions, revealing both the magnitude and direction of each feature's impact on the model's predictions, with color indicating the feature value (high or low) and horizontal position showing whether the feature pushes toward fraud (positive values) or legitimate (negative values).\n\n**2. Why are we doing this?**\n\nI'm conducting SHAP analysis because it offers deeper, more reliable global interpretability than simple feature importance measures. While LIME helped me understand individual predictions, SHAP provides a comprehensive view of how each feature impacts predictions across the entire dataset. This is crucial for fraud detection systems where explainability is as important as accuracy - stakeholders need to understand why certain job postings are flagged as fraudulent. SHAP values are based on solid game theory principles (Shapley values), ensuring fair attribution of each feature's contribution to predictions. This analysis helps me validate if my model is making predictions based on logical patterns rather than spurious correlations, builds trust in the system by making the decision process transparent, and provides actionable insights to both job posters (how to make their postings appear more legitimate) and platform administrators (what to look for when manually reviewing suspicious postings).\n\n**3. Analysis and insights**\n\nLooking at the SHAP summary plots from both training and test datasets, I can see consistent patterns indicating the model has learned stable, generalizable fraud indicators. The absence of a company logo (\"company_logo_no\") emerges as the most important feature in both plots, with a strong positive impact on fraud prediction. This makes logical sense - legitimate companies typically have established branding. Words like \"apply,\" \"entry,\" and \"motivated\" consistently push predictions toward fraud, aligning with what we know about scam language patterns - fraudulent postings often use action-oriented language targeting entry-level candidates. The absence of company profiles (\"company_profile_no\") also strongly correlates with fraud, confirming my earlier binary feature analysis. Interestingly, seeing both training and test distributions show the same pattern of feature importance confirms the model has captured real patterns rather than overfitting to training quirks. Industry-specific terms like \"engineering\" appear to push predictions away from fraud, suggesting technical job postings are less likely to be fraudulent.\n\n**4. Things to keep in mind**\n\nWhen interpreting these SHAP values, I need to consider several important factors. First, correlation doesn't imply causation - some features may correlate with fraud without directly causing it. For example, the absence of a company logo might correlate with fraud because many fraudulent postings lack logos, but legitimate startups might also lack polished branding. Second, feature interactions aren't fully captured in the summary plot - a feature's impact might depend on the presence or absence of other features. Third, the model might reflect historical biases in the training data - if certain industries or job types were disproportionately represented in fraudulent examples, the model might unfairly flag similar legitimate postings. Fourth, these insights represent patterns at the time of training - as fraudsters adapt their tactics, the importance of different features will likely shift. I should plan to periodically retrain the model and reassess feature importance. Finally, while SHAP provides valuable insights, it should complement rather than replace domain expertise when making final determinations about suspicious job postings.","metadata":{}},{"cell_type":"markdown","source":"# 9. Deep learning modelling (BERT)","metadata":{}},{"cell_type":"code","source":"# # Reference for the code below: https://colab.research.google.com/github/DerwenAI/spaCy_tuTorial/blob/master/BERT_Fine_Tuning.ipynb\n\n\n\n\nfrom transformers import (\n    BertTokenizer,\n    BertConfig,\n    BertForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom torch.utils.data import (\n    TensorDataset,\n    DataLoader,\n    RandomSampler,\n    SequentialSampler\n)\nfrom torch.optim import AdamW\nfrom sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n\nprint(\"=\" * 50)\nprint(f\"Starting job classification script at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 50)\n\n# ─────────────────────────────\n# ▶ Configuration\n# ─────────────────────────────\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\nMODEL_NAME = \"bert-base-uncased\"\nBATCH_SIZE = 16\nEPOCHS = 10  # Increased to 50 epochs\nMAX_LEN = 128\nLR = 2e-5\nFREEZE = True  # Set to True to freeze specified layers\nEARLY_STOPPING_PATIENCE = 3  # Number of epochs to wait before early stopping\n\nprint(f\"Model configuration:\")\nprint(f\"- Model: {MODEL_NAME}\")\nprint(f\"- Batch size: {BATCH_SIZE}\")\nprint(f\"- Epochs: {EPOCHS}\")\nprint(f\"- Max sequence length: {MAX_LEN}\")\nprint(f\"- Learning rate: {LR}\")\nprint(f\"- Freeze specified layers: {FREEZE}\")\n\n# ─────────────────────────────\n# ▶ Load Tokenizer\n# ─────────────────────────────\nprint(\"\\nLoading BERT tokenizer...\")\nbert_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"Tokenizer loaded: {bert_tokenizer.__class__.__name__}\")\n\n# ─────────────────────────────\n# ▶ Encoding Function\n# ─────────────────────────────\ndef encode_text_label(dataframe, text_field=\"text\", target_field=\"fraud\"):\n    print(f\"Encoding dataset with {len(dataframe)} samples...\")\n    start_time = time.time()\n    \n    tokenized_batch = bert_tokenizer(\n        dataframe[text_field].astype(str).tolist(),\n        padding='max_length',\n        truncation=True,\n        max_length=MAX_LEN,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    y_labels = torch.tensor(dataframe[target_field].values)\n    \n    print(f\"Encoding completed in {time.time() - start_time:.2f} seconds\")\n    print(f\"Encoded tensor shapes: Input IDs: {tokenized_batch['input_ids'].shape}, Attention mask: {tokenized_batch['attention_mask'].shape}, Labels: {y_labels.shape}\")\n    \n    # Print class distribution\n    class_counts = np.bincount(dataframe[target_field].values)\n    print(f\"Class distribution: {class_counts}\")\n    \n    return TensorDataset(tokenized_batch['input_ids'], tokenized_batch['attention_mask'], y_labels)\n\n# ─────────────────────────────\n# ▶ Dataset Prep\n# ─────────────────────────────\nprint(\"\\nPreparing datasets...\")\ntrain_data = encode_text_label(df_train_preprocessed)\nval_data = encode_text_label(df_validation_preprocessed)\ntest_data = encode_text_label(df_test_preprocessed)\n\nprint(f\"Creating data loaders...\")\ntrain_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=BATCH_SIZE)\nval_loader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=BATCH_SIZE)\n\nprint(f\"Data loaders created:\")\nprint(f\"- Training batches: {len(train_loader)}\")\nprint(f\"- Validation batches: {len(val_loader)}\")\nprint(f\"- Test batches: {len(test_loader)}\")\n\n# ─────────────────────────────\n# ▶ Load BERT Model\n# ─────────────────────────────\nprint(\"\\nLoading pre-trained BERT model...\")\nmodel_cfg = BertConfig.from_pretrained(MODEL_NAME)\nmodel_cfg.num_labels = 2  # Binary classification\nprint(f\"BERT config: {model_cfg}\")\n\nbert_classifier = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=model_cfg)\nbert_classifier.to(DEVICE)\nprint(f\"BERT classifier loaded and moved to {DEVICE}\")\n\n# ─────────────────────────────\n# ▶ Freeze base BERT layers, keep classifier trainable\n# ─────────────────────────────\n# if FREEZE:\n#     print(\"\\nFreezing base BERT layers, keeping classifier trainable...\")\n#     # Freeze all base BERT parameters\n#     for param in bert_classifier.bert.parameters():\n#         param.requires_grad = False\n    \n#     # Keep classifier layer trainable\n#     for param in bert_classifier.classifier.parameters():\n#         param.requires_grad = True\n\nif FREEZE:\n    for i in range(8): \n        for param in bert_classifier.bert.encoder.layer[i].parameters():\n            param.requires_grad = False\n\n    \n    # Count trainable vs frozen parameters\n    total_params = sum(p.numel() for p in bert_classifier.parameters())\n    trainable_params = sum(p.numel() for p in bert_classifier.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n    print(f\"Frozen parameters: {total_params - trainable_params:,} ({(total_params - trainable_params)/total_params:.2%})\")\n\n# ─────────────────────────────\n# ▶ Optimizer and Scheduler\n# ─────────────────────────────\nprint(\"\\nSetting up optimizer and learning rate scheduler...\")\noptimizer = AdamW(bert_classifier.parameters(), lr=LR, eps=1e-8)\nsteps_total = len(train_loader) * EPOCHS\nlr_schedule = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps_total)\nprint(f\"Optimizer: {optimizer.__class__.__name__}\")\nprint(f\"Learning rate scheduler: Linear with warmup\")\nprint(f\"Total training steps: {steps_total}\")\n\n# ─────────────────────────────\n# ▶ Set Random Seed\n# ─────────────────────────────\nSEED = 42\nprint(f\"\\nSetting random seed to {SEED} for reproducibility\")\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# ─────────────────────────────\n# ▶ Metrics Calculation Functions\n# ─────────────────────────────\ndef calculate_metrics(y_true, y_pred, y_scores=None):\n    \"\"\"Calculate various classification metrics\"\"\"\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n    \n    metrics = {\n        'TP': tp,\n        'TN': tn,\n        'FP': fp,\n        'FN': fn,\n        'Precision': precision,\n        'Recall': recall,\n        'F1': f1\n    }\n    \n    # Calculate AUC-ROC if probability scores are provided\n    if y_scores is not None:\n        try:\n            metrics['AUC-ROC'] = roc_auc_score(y_true, y_scores)\n        except:\n            metrics['AUC-ROC'] = float('nan')  # If only one class is present\n    else:\n        metrics['AUC-ROC'] = float('nan')\n        \n    return metrics\n\n# Create a list to store metrics for each epoch and dataset\nall_metrics = []\n\n# Create lists to track training and validation metrics for plotting\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\n# Variables for early stopping\nbest_val_loss = float('inf')\nearly_stopping_counter = 0\nearly_stopping = False\n\n# ─────────────────────────────\n# ▶ Training & Validation\n# ─────────────────────────────\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Starting training\")\nprint(\"=\" * 50)\n\nfor ep in range(EPOCHS):\n    print(f\"\\n--- Epoch {ep + 1} / {EPOCHS} ---\")\n    epoch_start_time = time.time()\n\n    # ─ Training ─\n    bert_classifier.train()\n    epoch_loss = 0\n    epoch_correct = 0\n    batch_count = 0\n    train_preds = []\n    train_true = []\n    train_scores = []\n\n    for batch_idx, batch in enumerate(train_loader):\n        if batch_idx % 10 == 0:\n            print(f\"  Training batch {batch_idx}/{len(train_loader)}\")\n            \n        ids, mask, labels = [t.to(DEVICE) for t in batch]\n\n        bert_classifier.zero_grad()\n        out = bert_classifier(ids, attention_mask=mask, labels=labels, return_dict=True)\n\n        out.loss.backward()\n        torch.nn.utils.clip_grad_norm_(bert_classifier.parameters(), max_norm=1.0)\n        optimizer.step()\n        lr_schedule.step()\n\n        epoch_loss += out.loss.item()\n        \n        # Get predictions and scores\n        logits = out.logits.detach().cpu()\n        preds = torch.argmax(logits, dim=1).numpy()\n        scores = torch.softmax(logits, dim=1)[:, 1].numpy()  # Positive class probability\n        labels_cpu = labels.cpu().numpy()\n        \n        train_preds.extend(preds)\n        train_true.extend(labels_cpu)\n        train_scores.extend(scores)\n        \n        epoch_correct += (preds == labels_cpu).sum()\n        batch_count += labels.size(0)\n\n    train_loss = epoch_loss / len(train_loader)\n    train_acc = epoch_correct / batch_count\n    \n    # Store metrics for plotting\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    \n    print(f\"Training completed in {time.time() - epoch_start_time:.2f} seconds\")\n    print(f\"Training Loss: {train_loss:.4f}\")\n    print(f\"Training Accuracy: {train_acc:.4f}\")\n    \n    # Calculate and store training metrics\n    train_metrics = calculate_metrics(train_true, train_preds, train_scores)\n    print(f\"Training Metrics:\")\n    print(f\"  - TP: {train_metrics['TP']}, TN: {train_metrics['TN']}, FP: {train_metrics['FP']}, FN: {train_metrics['FN']}\")\n    print(f\"  - Precision: {train_metrics['Precision']:.4f}\")\n    print(f\"  - Recall: {train_metrics['Recall']:.4f}\")\n    print(f\"  - F1: {train_metrics['F1']:.4f}\")\n    print(f\"  - AUC-ROC: {train_metrics['AUC-ROC']:.4f}\")\n    \n    all_metrics.append({\n        'Epoch': ep + 1,\n        'Model': MODEL_NAME,\n        'Dataset': 'Train',\n        **train_metrics\n    })\n\n    # ─ Validation ─\n    print(\"\\nRunning validation...\")\n    val_start_time = time.time()\n    bert_classifier.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    val_preds = []\n    val_true = []\n    val_scores = []\n\n    for batch_idx, batch in enumerate(val_loader):\n        if batch_idx % 5 == 0:\n            print(f\"  Validation batch {batch_idx}/{len(val_loader)}\")\n            \n        ids, mask, labels = [t.to(DEVICE) for t in batch]\n        with torch.no_grad():\n            res = bert_classifier(ids, attention_mask=mask, labels=labels, return_dict=True)\n        \n        val_loss += res.loss.item()\n        \n        # Get predictions and scores\n        logits = res.logits.detach().cpu()\n        preds = torch.argmax(logits, dim=1).numpy()\n        scores = torch.softmax(logits, dim=1)[:, 1].numpy()  # Positive class probability\n        labels_cpu = labels.cpu().numpy()\n        \n        val_preds.extend(preds)\n        val_true.extend(labels_cpu)\n        val_scores.extend(scores)\n        \n        val_correct += (preds == labels_cpu).sum()\n        val_total += labels.size(0)\n\n    val_acc = val_correct / val_total\n    val_loss = val_loss / len(val_loader)\n    \n    # Store metrics for plotting\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    \n    print(f\"Validation completed in {time.time() - val_start_time:.2f} seconds\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    print(f\"Validation Accuracy: {val_acc:.4f}\")\n    \n    # Early stopping check\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stopping_counter = 0\n        print(f\"New best validation loss: {best_val_loss:.4f}\")\n    else:\n        early_stopping_counter += 1\n        print(f\"Validation loss did not improve. Early stopping counter: {early_stopping_counter}/{EARLY_STOPPING_PATIENCE}\")\n        \n    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n        print(f\"\\nEarly stopping triggered after epoch {ep + 1}. Best validation loss: {best_val_loss:.4f}\")\n        early_stopping = True\n    \n    # Calculate and store validation metrics\n    val_metrics = calculate_metrics(val_true, val_preds, val_scores)\n    print(f\"Validation Metrics:\")\n    print(f\"  - TP: {val_metrics['TP']}, TN: {val_metrics['TN']}, FP: {val_metrics['FP']}, FN: {val_metrics['FN']}\")\n    print(f\"  - Precision: {val_metrics['Precision']:.4f}\")\n    print(f\"  - Recall: {val_metrics['Recall']:.4f}\")\n    print(f\"  - F1: {val_metrics['F1']:.4f}\")\n    print(f\"  - AUC-ROC: {val_metrics['AUC-ROC']:.4f}\")\n    \n    all_metrics.append({\n        'Epoch': ep + 1,\n        'Model': MODEL_NAME,\n        'Dataset': 'Validation',\n        **val_metrics\n    })\n\n    # Break the training loop if early stopping is triggered\n    if early_stopping:\n        break\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Training complete!\")\nprint(\"=\" * 50)\n\n# ─────────────────────────────\n# ▶ Test Evaluation\n# ─────────────────────────────\nprint(\"\\nEvaluating on Test Set...\")\ntest_start_time = time.time()\nbert_classifier.eval()\ntest_loss = 0\ntest_preds = []\ntest_true = []\ntest_scores = []\n\nfor batch_idx, batch in enumerate(test_loader):\n    if batch_idx % 5 == 0:\n        print(f\"  Test batch {batch_idx}/{len(test_loader)}\")\n        \n    input_ids, attention_mask, labels = [t.to(DEVICE) for t in batch]\n    with torch.no_grad():\n        result = bert_classifier(input_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n    \n    test_loss += result.loss.item()\n    \n    # Get predictions and scores\n    logits = result.logits.detach().cpu()\n    preds = torch.argmax(logits, dim=1).numpy()\n    scores = torch.softmax(logits, dim=1)[:, 1].numpy()  # Positive class probability\n    labels_cpu = labels.cpu().numpy()\n    \n    test_preds.extend(preds)\n    test_true.extend(labels_cpu)\n    test_scores.extend(scores)\n\ntest_accuracy = np.mean(np.array(test_preds) == np.array(test_true))\ntest_loss = test_loss / len(test_loader)\n\nprint(f\"Test evaluation completed in {time.time() - test_start_time:.2f} seconds\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Calculate and store test metrics\ntest_metrics = calculate_metrics(test_true, test_preds, test_scores)\nprint(f\"Test Metrics:\")\nprint(f\"  - TP: {test_metrics['TP']}, TN: {test_metrics['TN']}, FP: {test_metrics['FP']}, FN: {test_metrics['FN']}\")\nprint(f\"  - Precision: {test_metrics['Precision']:.4f}\")\nprint(f\"  - Recall: {test_metrics['Recall']:.4f}\")\nprint(f\"  - F1: {test_metrics['F1']:.4f}\")\nprint(f\"  - AUC-ROC: {test_metrics['AUC-ROC']:.4f}\")\n\nall_metrics.append({\n    'Epoch': EPOCHS,  # Use the final epoch number for test metrics\n    'Model': MODEL_NAME,\n    'Dataset': 'Test',\n    **test_metrics\n})\n\n# Print detailed classification report for test set\nprint(\"\\nClassification Report (Test Set):\")\nprint(classification_report(test_true, test_preds))\n\n# ─────────────────────────────\n# ▶ Create final metrics dataframe\n# ─────────────────────────────\nprint(\"\\nCreating final metrics dataframe...\")\nbert_metrics = pd.DataFrame(all_metrics)\n\n# Reorder columns for better readability\ncolumns_order = [\n    'Epoch', 'Model', 'Dataset', 'TP', 'TN', 'FP', 'FN',\n    'Precision', 'Recall', 'F1', 'AUC-ROC'\n]\nbert_metrics = bert_metrics[columns_order]\n\nprint(\"\\nFinal BERT Metrics DataFrame:\")\nprint(bert_metrics)\n\n# Save metrics to CSV\nmetrics_filename = f\"bert_job_classification_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\nbert_metrics.to_csv(metrics_filename, index=False)\nprint(f\"Metrics saved to {metrics_filename}\")\n\n# ─────────────────────────────\n# ▶ Plot training curves\n# ─────────────────────────────\nprint(\"\\nPlotting training and validation curves...\")\n\n# Create a figure with two subplots\nplt.figure(figsize=(15, 6))\n\n# Plot training and validation loss\nplt.subplot(1, 2, 1)\nepochs_range = range(1, len(train_losses) + 1)\nplt.plot(epochs_range, train_losses, 'b-', label='Training Loss')\nplt.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.grid(True)\nplt.legend()\n\n# Plot training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy')\nplt.plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# ─────────────────────────────\n# ▶ Save Model and Tokenizer\n# ─────────────────────────────\nmodel_save_dir = f\"./bert_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\nprint(f\"\\nSaving model and tokenizer to {model_save_dir}...\")\n\n# Create directory if it doesn't exist\nimport os\nos.makedirs(model_save_dir, exist_ok=True)\n\n# Save model and tokenizer\nbert_classifier.save_pretrained(model_save_dir)\nbert_tokenizer.save_pretrained(model_save_dir)\n\nprint(f\"Model and tokenizer successfully saved to {model_save_dir}\")\n\nprint(\"\\nTraining Summary:\")\nprint(f\"Total epochs run: {len(train_losses)}\")\nif early_stopping:\n    print(f\"Training stopped early at epoch {len(train_losses)} due to no improvement in validation loss\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\nprint(f\"Final training accuracy: {train_accuracies[-1]:.4f}\")\nprint(f\"Final validation accuracy: {val_accuracies[-1]:.4f}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Script completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 50)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T13:27:55.988856Z","iopub.status.busy":"2025-05-06T13:27:55.988563Z","iopub.status.idle":"2025-05-06T13:44:43.906690Z","shell.execute_reply":"2025-05-06T13:44:43.905957Z","shell.execute_reply.started":"2025-05-06T13:27:55.988835Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **1. What the code does so far?**\n\nI've implemented a deep learning approach to job posting fraud detection using BERT (Bidirectional Encoder Representations from Transformers) taking inpiration from h ttps://colab.research.google.com/github/DerwenAI/spaCy_tuTorial/blob/master/BERT_Fine_Tuning.ipynb and our professors Waad's classes . First, I load and configure a pre-trained BERT model (bert-base-uncased) and set it up for binary classification. To prevent overfitting on my relatively small dataset, I freeze the first 8 layers of the model while keeping the upper layers trainable - this preserves BERT's language understanding capabilities while allowing adaptation to my specific task. I then create a complete training pipeline that tokenizes the preprocessed job postings, encodes them as fixed-length sequences (128 tokens), and feeds them through the model. I've implemented early stopping with a patience of 3 epochs to prevent overfitting, along with an Adam optimizer and a linear learning rate scheduler. Throughout training, I track comprehensive metrics including precision, recall, F1-score, and AUC-ROC, storing them in a dataframe for later analysis. Finally, I evaluate the model on the test set, visualize learning curves, and save both the model and metrics for future use.\n\n**2. Why are we doing this?**\n\nI'm implementing BERT to leverage the power of transfer learning for my job posting classification task. Unlike the TF-IDF approach I used earlier, BERT captures contextual relationships between words and understands language at a deeper level through its pre-training on massive text corpora. This is crucial for fraud detection, where subtle linguistic cues and semantic patterns might indicate deception. BERT can recognize that phrases like \"earn money from home\" have different implications than \"work remotely\" even though both discuss working from home - something bag-of-words models struggle with. By freezing the lower layers, I'm preserving BERT's fundamental language understanding while allowing the upper layers to adapt to my specific task, striking a balance between leveraging pre-trained knowledge and preventing overfitting. This approach should capture more nuanced patterns in fraudulent language that simpler models might miss, potentially leading to higher recall of fraudulent postings without sacrificing precision.\n\n**3. Analysis and insights**\n\nMy BERT implementation shows impressive performance, achieving 86% F1-score on the test set with excellent balance between precision (87.9%) and recall (84.2%). The learning curves reveal that the model learns quickly, with significant improvements in the first few epochs before triggering early stopping at epoch 6 due to no further validation loss improvement. The final test AUC-ROC of 0.987 indicates the model's excellent ability to distinguish between classes, far outperforming traditional models. Looking at the training progression, I can see that by epoch 6, the model achieved nearly perfect training metrics (98.7% F1-score), while maintaining strong validation performance (88.9% F1-score), suggesting good generalization. The early stopping mechanism worked effectively, preventing overfitting when validation performance plateaued. Interestingly, the model seems to prioritize precision over recall at test time (87.9% vs 84.2%), meaning it's slightly more cautious about flagging postings as fraudulent, which is appropriate for this application where false positives could harm legitimate employers.\n\n**4. Things to keep in mind**\n\nWhile my BERT implementation performs well, I need to be mindful of several factors. First, the computational demands are substantial - training took significantly longer than traditional models and requires more resources, which may impact deployment considerations. Second, though I implemented class balancing for my traditional models, I didn't specifically address class imbalance for BERT; exploring weighted loss functions might further improve performance on the minority fraud class. Third, the sequence length limitation (128 tokens) means longer job postings get truncated, potentially losing valuable information - experimenting with longer sequences or custom truncation strategies could help. Fourth, comparing these results with my TF-IDF based models would be essential to determine if the additional complexity of BERT is justified by performance gains. Finally, I should consider ensemble approaches that combine BERT's semantic understanding with the explicit feature-based insights from my traditional models, potentially leveraging the strengths of both approaches. These hybrid models might provide both the contextual understanding of deep learning and the interpretability of traditional approaches.","metadata":{}},{"cell_type":"markdown","source":"# 10.  BERT Interpretation","metadata":{}},{"cell_type":"code","source":"\n\n# Create wrapper function for the BERT model\ndef predict_proba_bert(text_array):\n    \"\"\"Wrapper for BERT model prediction\"\"\"\n    # Tokenize the texts\n    encoded_texts = bert_tokenizer(\n        text_array,\n        padding='max_length',\n        truncation=True,\n        max_length=MAX_LEN,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    \n    # Move inputs to the device\n    input_ids = encoded_texts['input_ids'].to(DEVICE)\n    attention_mask = encoded_texts['attention_mask'].to(DEVICE)\n    \n    # Get predictions\n    bert_classifier.eval()\n    with torch.no_grad():\n        outputs = bert_classifier(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n    \n    # Convert logits to probabilities\n    probs = torch.softmax(logits, dim=1).cpu().numpy()\n    return probs\n\n# Initialize LIME explainer\nexplainer = LimeTextExplainer(class_names=['Non-Fraud', 'Fraud'])\n\n# Get one fraud and one non-fraud example from test set\ntest_df = df_test_preprocessed  # Update with your test dataframe name if different\nfraud_idx = test_df[test_df['fraud'] == 1].index[1]\nnon_fraud_idx = test_df[test_df['fraud'] == 0].index[0]\n\n# Explain non-fraud example\nprint(\"Explaining non-fraud example...\")\nnon_fraud_text = test_df.loc[non_fraud_idx, 'text']\nprint(non_fraud_text)\nnon_fraud_explanation = explainer.explain_instance(\n    non_fraud_text, \n    predict_proba_bert,\n    num_features=20,\n    num_samples=100\n)\n# Display the explanation\nprint(\"Non-fraud explanation:\")\nnon_fraud_explanation.show_in_notebook(text=True)\n\n","metadata":{"execution":{"iopub.execute_input":"2025-05-06T14:14:48.947416Z","iopub.status.busy":"2025-05-06T14:14:48.946676Z","iopub.status.idle":"2025-05-06T14:14:49.595820Z","shell.execute_reply":"2025-05-06T14:14:49.594968Z","shell.execute_reply.started":"2025-05-06T14:14:48.947391Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"non_fraud_explanation.as_list(label=1)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T14:36:16.684571Z","iopub.status.busy":"2025-05-06T14:36:16.684099Z","iopub.status.idle":"2025-05-06T14:36:16.689312Z","shell.execute_reply":"2025-05-06T14:36:16.688752Z","shell.execute_reply.started":"2025-05-06T14:36:16.684545Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explain fraud example\nprint(\"Explaining fraud example...\")  \nfraud_text = test_df.loc[fraud_idx, 'text']\nfraud_explanation = explainer.explain_instance(\n    fraud_text, \n    predict_proba_bert,\n    num_features=20,\n    num_samples=100  # Increased samples for better stability\n)\n# Display the explanation\nprint(\"Fraud explanation:\")\nfraud_explanation.show_in_notebook(text=True)\n\n# # Save the explanations as HTML files (optional)\n# non_fraud_html = f\"bert_lime_non_fraud_explanation.html\"\n# fraud_html = f\"bert_lime_fraud_explanation.html\"\n# non_fraud_explanation.save_to_file(non_fraud_html)\n# fraud_explanation.save_to_file(fraud_html)\n# print(f\"Explanations saved to {non_fraud_html} and {fraud_html}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-06T16:01:10.999601Z","iopub.status.busy":"2025-05-06T16:01:10.999025Z","iopub.status.idle":"2025-05-06T16:01:11.648414Z","shell.execute_reply":"2025-05-06T16:01:11.646997Z","shell.execute_reply.started":"2025-05-06T16:01:10.999575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fraud_explanation.as_list(label=1)","metadata":{"execution":{"iopub.execute_input":"2025-05-06T16:05:37.529180Z","iopub.status.busy":"2025-05-06T16:05:37.528871Z","iopub.status.idle":"2025-05-06T16:05:37.535552Z","shell.execute_reply":"2025-05-06T16:05:37.534711Z","shell.execute_reply.started":"2025-05-06T16:05:37.529149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. What the code does so far?**\n\nI've implemented LIME (Local Interpretable Model-agnostic Explanations) for my BERT classifier to understand exactly which parts of a job posting influence the model's fraud predictions. I created a wrapper function that handles the BERT-specific tokenization process, converts inputs to the appropriate tensor format, and returns probability scores. This allows LIME to treat the complex BERT model as a simple function it can query repeatedly. For each example (one fraudulent and one non-fraudulent job posting), I generate explanations with 20 features and 100 sampling iterations, which helps identify the most influential words and features in each document. The code visualizes these explanations by highlighting text based on its contribution to either class – red for features pushing toward fraud classification and blue for features pushing toward legitimate classification. This approach provides word-level granularity in understanding how the deep learning model makes decisions, something not inherently available in transformer-based models like BERT.\n\n**2. Why are we doing this?**\n\nI'm applying LIME to my BERT model because deep learning models, while powerful, typically function as black boxes that provide little insight into their decision-making process. By using LIME, I can peek inside this black box and understand which specific words or features in a job posting trigger the model's fraud detection. This interpretability is crucial for several reasons: it helps build trust in the model's decisions, allows me to verify that the model is focusing on meaningful patterns rather than spurious correlations, and provides actionable insights to platform administrators about the linguistic cues of fraudulent postings. While global feature importance (through SHAP) showed us overall patterns across the dataset, LIME allows me to understand specific predictions at the individual posting level. This combination of approaches gives me both the bird's-eye view of fraud patterns and the detailed examination of specific cases, creating a more comprehensive understanding of how my model works.\n\n**3. Analysis and insights**\n\nLooking at the LIME explanations reveals fascinating patterns in how my BERT model interprets job postings. In the non-fraudulent example, words like \"implement,\" \"users,\" and \"women\" push strongly toward the legitimate classification, suggesting that specific action verbs and target audience mentions are associated with legitimate postings. The feature \"company_logo_yes\" also pushes toward legitimacy, confirming my earlier binary feature analysis. For the fraudulent posting, terms like \"based,\" \"positions,\" \"access,\" \"special,\" \"clerks,\" \"internet,\" and \"city_aberdeen\" contribute to the fraud classification. This suggests the model has learned to identify specific location patterns and suspicious terminology combinations typically found in scam postings. Interestingly, \"company_profile_no\" in the fraudulent example actually pushes slightly away from the fraud class, contradicting my previous findings – this might indicate that BERT is capturing more complex interaction patterns beyond simple feature presence or absence. While these explanations provide valuable insights, the relatively small coefficients suggest that BERT's decisions are based on subtle patterns across many features rather than a few dominant signals.\n\n**4. Things to keep in mind**\n\nWhile these LIME explanations provide helpful insights, I need to be aware of several limitations. First, the current analysis used only 100 samples and 20 features – increasing both would provide more stable and comprehensive interpretations, though at the cost of longer computation time. The small coefficients in the current results suggest that more samples might help determine feature importance with greater confidence. Second, my BERT model showed excellent performance on the test set with minimal overfitting, but I could potentially improve it further by increasing training epochs beyond the early stopping point or implementing data augmentation techniques specific to text classification. Third, the interpretations might be affected by BERT's tokenization process, which breaks words into subwords – what appears as a single influential word might actually be a combination of subword tokens. Finally, I should compare these LIME explanations with my earlier SHAP analysis to see if both interpretation methods highlight similar important features, which would strengthen confidence in my understanding of the model's decision-making process. This multi-method approach to interpretability can provide a more robust understanding of how my fraud detection system works.","metadata":{}},{"cell_type":"markdown","source":"# **Conclusion**\n\nThroughout this job posting fraud detection project, I've explored a comprehensive journey from data exploration to advanced deep learning. Starting with feature engineering and statistical analysis, I discovered clear patterns distinguishing fraudulent from legitimate postings - from missing company logos to suspicious word choices and geographic distributions. My approach evolved from traditional machine learning with TF-IDF vectorization to implementing BERT, which captured subtle linguistic nuances beyond simple keyword matching. The interpretability techniques I applied (LIME and SHAP) opened the black box of these models, providing actionable insights about fraud indicators that could help protect job seekers.\n\nPersonally, I found the balance between model performance and interpretability particularly fascinating. While BERT achieved impressive results (86% F1-score), understanding its decision-making process proved equally valuable. This project reinforced my belief that in high-stakes applications like fraud detection, we must not only build accurate models but also explain their decisions transparently.\n\nI'd like to express my sincere gratitude to you Professor Waad for the insightful classes that provided the foundation for this work. The concepts and techniques learned throughout the course proved invaluable in tackling this real-world problem. This project has deepened my appreciation for the power of NLP in addressing meaningful challenges that impact people's lives.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}